<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.1">Jekyll</generator><link href="http://localhost:4000/atom.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-06-05T00:31:21-04:00</updated><id>http://localhost:4000/atom.xml</id><title type="html">Jake Zimmerman</title><subtitle>A collection of blog posts about programming, software, types, programming languages, Sorbet, Vim, Markdown, and more.</subtitle><author><name>Jake Zimmerman</name></author><entry><title type="html">With types, seeing is believing</title><link href="http://localhost:4000/seeing-is-believing/" rel="alternate" type="text/html" title="With types, seeing is believing" /><published>2022-06-04T23:31:51-04:00</published><updated>2022-06-04T23:31:51-04:00</updated><id>http://localhost:4000/seeing-is-believing</id><content type="html" xml:base="http://localhost:4000/seeing-is-believing/">From time to time I get asked something like this:

&gt; I write a lot of Ruby at work. In past projects I've really enjoyed and benefited from
&gt; a statically typed language. But on my current team, people aren't as excited.
&gt;
&gt; What can I say to them to get them to change their mind, so we can start using types in
&gt; Ruby?

In my experience, this framing is backwards. You don't first convince everyone that types
are good, and then start adopting types. Instead, you put them to sleep, enter their
dream, and plant the idea that types are good—ah, wait, wrong storyline. Instead, you adopt
types first and then let people see for themselves what types do for them. The people
opposed to types won't be convinced to start liking them by anything you can tell them or
ask them to read.

&lt;!-- more --&gt;

In all of the cases where I've seen Sorbet be adopted, the process looked like
this:

1. An ambitious team (or even individual) who really, really wants types in Ruby does the
   work to get an initial pass at adoption passing in CI.[^grind] Importantly, the initial
   pass does a minimal amount of work, so that it doesn't take long to get here.

   For [Sorbet], that means only checking at [`# typed: false`], which enables Sorbet in
   every file but only does the most basic checks, like checking for syntax errors and
   typos in constant literals.

1. That initial version sits silently in the codebase over a period of days or weeks. When
   new changes introduce new type errors, it pings the enthusiastic types adoption team;
   they figure out whether it caught a real bug or whether the tooling could be improved
   (for example, for syncing type definitions for third-party code). It does **not** ping
   the unsuspecting user yet.

   When we did this to roll out Sorbet at Stripe, this manifested as a job that ran on the
   `master` branch[^master] in CI, but if it failed it would send a Slack message to us,
   not tell the user that their change was broken.

1. This process repeats until the pings are only **high-signal** pings. When there's an
   error, it represents actual bugs (or maybe doesn't ping: remember, most files are still
   `# typed: false`).

1. Double check at this point that it's easy to configure whatever editors your team uses
   to put the errors directly in the editor. You likely already did this for yourself
   while working on the initial migration.

   Sorbet exposes an [LSP server] via the `--lsp` command line flag to allow integrating
   with arbitrary editors, and also publishes a [VS Code extension] for people who want a
   one-click, low-config solution.

1. The time has come to enforce that the codebase type checks in CI. You and your team
   effectively beta-tested it on behalf of the organization and decided it wasn't going to
   bring development to a halt. Flip the switch, and if need be, remind people that this
   is still an experiment. &quot;We can try it out for a while and re-evaluate later—it's still
   the same language.&quot;

   Most code still has no explicit type annotations and limited type checking (due to the
   `# typed: false`), but now more teams can experiment with enabling stricter type
   checking in the sections of the codebase they own.[^gradual]

1. Finally, the important part: **show** people how good Sorbet is, don't tell them. Fire
   up Sorbet on your codebase, delete something, and watch as the error list populates
   instantly. Jump to definition on a constant. Try autocompleting something.

   Notice how we're **not** showing off the type system and how expressive it might
   be. We're showing off what the type system actually lets them do! Be more productive at
   their job.

[^grind]:
  This can be somewhat of a grind, and is all too often done during nights and weekends,
  though high-trust teams do a good job of carving out time for experiments like this.

[^master]:
  Limiting to `master`, instead of all branches, is a convenient way to get a sense for
  whether enforcing types would have actually blocked someone. It's far more likely that
  in-progress branches with type failures also have failing tests, and that the type
  failure would have done a _better_ job at alerting the user to the problem.

[^gradual]:
  This is the whole point of &quot;gradual&quot; in [gradual type checking].

In my experience trying to bring static types to Ruby users, seeing is really believing.
I've seen this exact same story or slight variations of it play out in just about
every successful adoption case.

While it's true that the type checker is going to prevent people from writing valid code
they used to be able to write, every gradual type system has [escape hatches] to opt out
of those checks in some way. But it's the feeling of instant feedback and powerful editor
features that's impossible to convince someone of until they've had the chance to see it
in their own editor, on the files they work with.

One final, important note: **be supportive**. Advertise a single place for anyone to ask
questions and get quick responses.[^quick] Admit that this will likely lead to being
overworked for a bit until it takes off. In the long run as adoption and experience using
the type checker spreads, other teammates will start to help out with the evangelism as
the benefits spread outward.

[^quick]:
  Like, actually quick. &quot;Notify for new Slack every message&quot; quick. If you queue questions
  into some ticketing system and respond tomorrow, people will lose patience with _types
  overall_ not just with you.

[Sorbet]: https://sorbet.org
[`# typed: false`]: https://sorbet.org/docs/static#file-level-granularity-strictness-levels
[LSP Server]: https://microsoft.github.io/language-server-protocol/
[VS Code extension]: https://sorbet.org/docs/vscode
[gradual type checking]: https://sorbet.org/docs/gradual
[escape hatches]: https://sorbet.org/docs/troubleshooting#escape-hatches</content><author><name>Jake Zimmerman</name></author><category term="sorbet" /><category term="devprod" /><summary type="html">← Return home From time to time I get asked something like this:</summary></entry><entry><title type="html">Is tree-sitter good enough?</title><link href="http://localhost:4000/tree-sitter-limitations/" rel="alternate" type="text/html" title="Is tree-sitter good enough?" /><published>2022-05-30T04:43:46-04:00</published><updated>2022-05-30T04:43:46-04:00</updated><id>http://localhost:4000/tree-sitter-limitations</id><content type="html" xml:base="http://localhost:4000/tree-sitter-limitations/">**tl;dr**: no, or at the very least, &quot;not for every use case.&quot; (Though I really wish it
were for the use cases I have, because it would save me a lot of work.)

&lt;!-- more --&gt;

&gt; I'm guessing you already know what tree-sitter is because you clicked on the title. If
&gt; you clicked because you were hoping to find out: [tree-sitter] is a relatively[^new] new
&gt; project which aims to make writing fast, error-tolerant parsers take less work. To do
&gt; that, it provides both pre-built parsers for common programming languages and a toolkit
&gt; for building new parsers. It's known for use in various GitHub features by way of their
&gt; [semantic] tool, which powers the code navigation tooltips that you sometimes see on
&gt; GitHub.[^semantic]

[^new]:
  Is it still new? The GitHub repo has commits dating back to 2013, though I only
  first heard about it in 2017. It still has a feeling of newness about it, but I digress.

[^semantic]:
  The semantic repo actually has a [short overview][why-tree-sitter] of why they chose
  tree-sitter, along with some drawbacks.

For a lot of projects, tree-sitter is really nice! _Especially_ for projects where the
quality of the parser is less important than the quantity of languages supported. For
example: an editor syntax highlighter. It's more important that the editor highlight lots
of languages' syntax than it is that every language is highlighted perfectly. Another
example: building something like [ParEdit] for arbitrary languages. Or providing
jump-to-def that's mostly better than plain-text code search.[^approval] For a lot of
these applications it's actually _completely fine_ if there's a flagrant bug in one of the
grammars, because the project is still so useful in all the other languages.

[^approval]:
  Another neat use case, from work: every time a commit is pushed to an approved PR, the
  approval is dismissed, unless (using tree-sitter) the CI system detects that the parse
  tree hasn't changed. This spares comment and formatting changes the toil of a re-review.

But when the goals are flipped—it has to work for exactly one language, and the quality of
the parser is paramount—tree-sitter becomes less attractive. There are two questions I
would pose to anyone curious about using tree-sitter for their parser:

1.  Is serving autocompletion requests a key use cases?

    Serving autocompletion requests requires an unnaturally high parse fidelity, even when
    the buffer is ridiculed with syntax errors.

2.  How much do you care about crafting custom messages for syntax errors?

    Customizing syntax error messages becomes context-dependent very quickly. It's easy to
    maintain that context when your parser allows running arbitrary code, and hard when
    the parser is constrained to a declarative DSL.

If either of these goals are important, I'd recommend rolling your own parser (using the
technique of your choice). It comes down to flexibility: a tree-sitter grammar, with it's
declarative specification, provides a lot of neat features for free (like error recovery),
but places a ceiling on possibilities for future improvement.

Let me show some examples.[^bugs] The snippets of code below are exactly the kinds of
programs that people type in their editors, but which tree-sitter doesn't parse well
enough. You can follow along on the [tree-sitter online playground].

[^bugs]:
  It's entirely possible that I've just been _really_ unlucky, and that the problems I've
  found are all fixable with a few bug reports and a little ingenuity. But if it's
  going to take ingenuity anyways, isn't that the same as writing a parser myself?

\

Let's start with a Ruby program, alongside its parse result:

```ruby
f = -&gt;(x) {
  x.
}
```

```{.numberLines .hl-8 .hl-9}
program [0, 0] - [3, 0]
  assignment [0, 0] - [2, 1]
    left: identifier [0, 0] - [0, 1]
    right: lambda [0, 4] - [2, 1]
      parameters: lambda_parameters [0, 6] - [0, 9]
        identifier [0, 7] - [0, 8]
      body: block [0, 10] - [2, 1]
        identifier [1, 2] - [1, 3]
        ERROR [1, 3] - [1, 4]
```

Here's what a comparable, syntactically-valid parse looks like:

```ruby
f = -&gt;(x) {
  x.foo
}
```

```{.numberLines .hl-8 .hl-9 .hl-10}
program [0, 0] - [3, 0]
  assignment [0, 0] - [2, 1]
    left: identifier [0, 0] - [0, 1]
    right: lambda [0, 4] - [2, 1]
      parameters: lambda_parameters [0, 6] - [0, 9]
        identifier [0, 7] - [0, 8]
      body: block [0, 10] - [2, 1]
        call [1, 2] - [1, 7]
          receiver: identifier [1, 2] - [1, 3]
          method: identifier [1, 4] - [1, 7]
```

In the good parse, tree-sitter produces a `call` node. In the bad parse, it just produces
a `block` that has a list containing two elements. Ideally, what we'd see here is
something like this:

```
call
  receiver: identifier
  method: ERROR
```

Which tells us that there was a method call, what the receiver of the method call was so
we know where to start looking for methods to autocomplete, and that the syntax error was
localized to the method call.

There's a similar problem with constant accesses:

```ruby
f = -&gt;(x) {
  A::
}
g = -&gt;(x) {
  A::B
}
```

```{.numberLines}
program [0, 0] - [6, 0]
  # ...
      body: block [0, 10] - [2, 1]
        constant [1, 2] - [1, 3]
        ERROR [1, 3] - [1, 5]
  # ...
      body: block [3, 10] - [5, 1]
        scope_resolution [4, 2] - [4, 6]
          scope: constant [4, 2] - [4, 3]
          name: constant [4, 5] - [4, 6]
```

... and a similar problem with `@` (the start of an instance variable access), and with `x =` (the start of an assignment).

\

Maybe this example was a little contrived? Comparable programs written in JavaScript
actually parse the good way, so maybe that's just an indictment of tree-sitter-ruby, not
tree-sitter itself.

But this next snippet reproduces in both Ruby and JavaScript:

```js
class A {
  foo() {

  bar() {
  }
}
```

```
program [0, 0] - [6, 0]
  class_declaration [0, 0] - [5, 1]
    name: identifier [0, 6] - [0, 7]
    body: class_body [0, 8] - [5, 1]
      member: method_definition [1, 2] - [4, 3]
        name: property_identifier [1, 2] - [1, 5]
        parameters: formal_parameters [1, 5] - [1, 7]
        body: statement_block [1, 8] - [4, 3]
          expression_statement [3, 2] - [3, 9]
            call_expression [3, 2] - [3, 7]
              function: identifier [3, 2] - [3, 5]
              arguments: arguments [3, 5] - [3, 7]
            ERROR [3, 8] - [3, 9]
```

To make it more obvious why this parse tree is not great, it's basically the same parse
tree as produced by this program:

```js
class A {
  foo() {
    bar();
    {
  }
}
```

Some points:

- Even though `bar() { ... }` is valid method syntax, there's no definition of a method
  called `bar` in the parse. Instead, the parser thinks that there was a **function call**
  to a function named `bar` that doesn't exist.
- The syntax error shows up after the imagined call to `bar` (associated with the `{`
  immediately after the call to `bar`), not associated with the `foo` method.

If the user's cursor was inside the `bar` method and asking for completion results, we'd
be forced to serve them completion results as though their cursor was inside the
half-formed `foo` method, which produces completely wrong results.

This behavior is not unique to JavaScript. I've reproduced it almost verbatim in Ruby and
Java, and partially in most other tree-sitter parsers (C#, C++, Rust, etc.).

The best behavior here would be to point out that the curly braces are mismatched,[^rust]
and then recover assuming that the user fixed that mismatch, preserving the `bar` method.

[^rust]:
  Indeed, that's [exactly the error](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2021&amp;gist=872bd946a8789aba9d49e07aef614819)
  on a comparable Rust example. (Rust's parser is hand-written.)

\

I could turn this into a post full of weird code snippets and poor parse results, but
that's not useful. What I'm trying to show is that when the demands are, &quot;The one specific
language I care about has lots of idiosyncratic but common parse errors that I want to
handle well,&quot; then prepare to devote a substantial amount of time to tweaking anyways. I
prefer doing that in a setting that gives me maximum flexibility, so that I can be as
clever as I need to eke out good parse results.

Don't get me wrong, I still think tree-sitter is a great project with a neat new idea. I
also haven't shown how surprisingly good tree-sitter was on a lot of the examples I tried!
All I'm saying is that tree-sitter comes with tradeoffs, and that it's not useful to
respond to every complaint about an existing parser with, &quot;If you just used tree-sitter,
your problems would go away,&quot; because that's not true. For a certain class of parsing
problems, tree-sitter is not quite good enough.

\

*If I've overlooked something, please let me know and I'll happily update this post (and
maybe even start using tree-sitter in my projects).*


[tree-sitter]: https://tree-sitter.github.io/tree-sitter/
[semantic]: https://github.com/github/semantic
[why-tree-sitter]: https://github.com/github/semantic/blob/master/docs/why-tree-sitter.md
[ParEdit]: https://www.emacswiki.org/emacs/ParEdit
[tree-sitter online playground]: https://tree-sitter.github.io/tree-sitter/playground
[craft a bug report here]: https://sorbet.run/?arg=--print=parse-tree-whitequark#%23%20typed%3A%20true%0A%23%20Share%20your%20example%20with%20%22Examples%20%3E%20Create%20issue%20with%20example%22%0A</content><author><name>Jake Zimmerman</name></author><category term="parsing" /><category term="tree-sitter" /><summary type="html">← Return home tl;dr: no, or at the very least, “not for every use case.” (Though I really wish it were for the use cases I have, because it would save me a lot of work.)</summary></entry><entry><title type="html">What would a type-aware Rubocop look like?</title><link href="http://localhost:4000/type-aware-rubocop/" rel="alternate" type="text/html" title="What would a type-aware Rubocop look like?" /><published>2022-05-16T16:17:20-04:00</published><updated>2022-05-16T16:17:20-04:00</updated><id>http://localhost:4000/type-aware-rubocop</id><content type="html" xml:base="http://localhost:4000/type-aware-rubocop/">_This post represents my opinions at a point in time. It's not necessarily the views of my
team or my employer._

From time to time, someone asks, &quot;Would [Sorbet] ever allow defining some sort of type-aware
lint rules?&quot; The answer has usually been &quot;no,&quot; for a couple of reasons.

[Sorbet]: https://sorbet.org

&lt;!-- more --&gt;

The biggest open question is that it's not 100% clear what use cases people have in mind.
Most commonly people imagine &quot;the full [Rubocop] API, but with types,&quot; but this is
underspecified, in my opinion.

[Rubocop]: https://rubocop.org/

### Should every AST node (i.e., every expression) have a type associated with it?

This would be particularly hard to support, because Sorbet aggressively simplifies the
AST from the start to the end of its pipeline. The Rubocop AST has something like 100
node types. Sorbet immediately simplifies this into an AST that only has 30 or so node
types, then subsequently keeps refining the AST until it only has about 15. The thing
Sorbet type checks looks nothing like the AST that you'd want if you were trying to write
a linter, because so much of it has been desugared, rewritten, or simplified.

Then finally right before type checking, Sorbet actually abandons the (tree-based) AST,
preferring to use a **control flow graph** (CFG) for type checking! A CFG is no longer
tree-based[^graph], which breaks a lot of the assumptions people make about what's
easy and hard to build in a linter rule.

[^graph]:
  It's a graph, where nodes are basic blocks and edges are control flow jumps between
  those blocks.

Because Sorbet type checks a CFG only, there's no tree-based structure inside Sorbet that
has types. _Maybe_ it's possible to take the type-annotated CFG and use it to reconstruct
some sort of typed AST, but that sounds brittle and error prone.

And finally, in the CFG Sorbet doesn't associate types with expressions, only types with
variables! This works because the act of building a CFG assigns all intermediate
expressions' results to a variable and then only dealing with variables from then on.

### Maybe types for just variables is enough?

This would likely be somewhat easier to implement, because Sorbet already does maintain
environments mapping variables to their types.

However, these data structures are expensive to maintain and therefore not long-lived.[^lsp]
Unlike Sorbet's symbol table, which exists indefinitely after creation, the environments
that track variable types only last as long as is required to type check a single
method.

[^lsp]:
  Sorbet's LSP editor integration gets around this by re-typechecking an entire method
  every time the user hovers over a variable. When these hover requests come as
  (infrequent) requests from the user, this is fine because Sorbet is already initialized.
  Powering a linter this way would either require that the Sorbet server be
  initialized for variable type every request (crazy slow), or somehow kept around
  persistently (brittle).

Maybe there could be an API like &quot;please give me the type of the variable with this name,&quot;
but again this would be tricky, because chances are the lint rule author wants to build
the lint rule on some tree-based data structure, and Sorbet only has the CFG. So there
would additionally need to be some mapping between environments (basic blocks) and AST
variable nodes, which again sounds pretty tricky and likely to break some assumptions.

### Even if this &quot;give me the type of a variable&quot; API works, is it enough?

Knowing the type of a variable on its own isn't very useful. The most common questions you
want ask of a type are:

1.  Is is this type a subtype of this other type?
1.  If a method with a given name is called on a receiver of this type, what are the
    _list_ of methods that would be dispatched to? (It's a list because the receiver could
    be a union type.)

The answer to (1) requires having the entire symbol table on hand (lots of memory). The
answer to (2) is subtle and complicated—Sorbet spends [about 4,000 lines of
code][calls.cc] answering it—and _also_ requires having the symbol table on hand.

[calls.cc]: https://github.com/sorbet/sorbet/blob/master/core/types/calls.cc

So it's probably not enough to just, e.g., return some JSON representation of one of
Sorbet's types. It'd also require having some structured representation of Sorbet's symbol
table, which brings us to our next question:

### Rubocop + types, or Sorbet + linter?

So far I've kind of assumed that we want to start with an existing linter (Rubocop) and
just add types. But what we've seen so far is that the things we'd need to get types into
Rubocop basically amount to exporting almost all of Sorbet's internal data structures.

Sorbet's internal data structures change all the time as we fix bugs, add features, and
refactor things. Having to commit to a stable API for every internal data structure
mentioned above would slow down how quickly we can improve the rest of Sorbet.

So maybe instead of exporting an API that Rubocop could use, we should build a linter into
Sorbet? This just has different tradeoffs:

- Sorbet has to reinvent the wheel on linter APIs (e.g., are lint rules specified in Ruby
  code with some new API? Does it attempt to copy as much of Rubocop's API as possible?
  What happens when there are papercut differences between what Sorbet's linter allows and
  what Rubocop does?)

- How are rules distributed? Are the rules written in Ruby, and Sorbet runs the Ruby code
  with some sort of FFI to expose the internal data structures? Does Sorbet embed some
  other scripting language for writing rules? Do people write rules as shared objects
  which Sorbet dynamically loads, akin to Ruby native extensions? Are the rules committed
  directly into the Sorbet repo, like how custom DSL and rewriter passes are right now?

### ... what do people actually want this for?

Whenever someone asks for a type-aware linter, here are a sampling of the answers given
when I ask, &quot;What are you really trying to do?&quot;

1.  &quot;Ban calling `to_s` on `nil`, because I just spent half an hour tracking down a bug
    where I had a `T.nilable(Symbol)` that I called `to_s` on and got the wrong answer.&quot;

1.  &quot;Update the [Performance/InefficientHashSearch] rule to only act on `Hash` values. Not
    all calls like `xs.values.include?` can be safely rewritten to `xs.value?`.&quot;

1.  &quot;Do type-aware codemods, for example change all calls to `x.merchant_` to something
    like `ClassOfX.get_merchant(x.id)`&quot;

1.  &quot;Enforce that all methods returning `T::Boolean` have names ending in `?`&quot;

[Performance/InefficientHashSearch]: https://docs.rubocop.org/rubocop-performance/cops_performance.html#performanceinefficienthashsearch

It's not clear that &quot;just&quot; building a type-aware linter necessarily solves these problems.

Doing (1) is hard—should we allow `Object#to_s`? You could still accidentally call `to_s` on
something that's `nil` inside a method that accepts `Object` if you do. Also there are
sometimes valid cases to call `to_s` on `nil` that no type system will help you discover!
This feature seems similar to the `#poison` pragma in C and C++, but there the language
makes it easier because `#include`'ed files are explicitly ordered, and it's easy to say
&quot;after this point, the identifier is poisoned.&quot; (Also I'm not even sure how `#poison`
works with methods, not just C functions, where things like inheritance become a problem.)

Doing (2) relies on that hard feature we chatted about above: types for arbitrary
expressions, not just variables. If we don't have types for arbitrary expressions,
detecting this case in a cop requires essentially re-inventing Sorbet's inference
algorithm: `input.map {...}.filter {...}.values.include?`. We mentioned the difficulty in
exposing types for arbitrary expressions above.

The situation for (3) is something I can really relate to, as there are a lot of cases
where I can imagine this being useful. But rather than build this as a lint rule, we've
historically wanted to build these as IDE-mode code actions: the API is much more
constrained (no internal data structures needed) and the IDE already has the type
information in memory. Sorbet supports a limited number of refactorings now, but mostly
because we haven't spent time on it. It's reasonable to assume we'll build many more
refactorings in the future.

And finally, things like (4) can _already_ be done in Rubocop. It's slightly more annoying
(you have to write the code to parse Sorbet signature annotations manually) but Sorbet
signature annotations are very stable. Their syntax changes infrequently, and when it
does, it's usually minor and/or backwards compatible changes.

### ... is there anything like this in another language?

Here's one of Sorbet's explicit design principles:

&gt; 3.  As simple as possible, but powerful enough
&gt;
&gt;     Overall, we are not strong believers in super-complex type systems. They have their
&gt;     place, and we need a fair amount of expressive power to model (enough) real Ruby
&gt;     code, but all else being equal we want to be simpler. We believe that such a system
&gt;     scales better, and—most importantly—is easier for our users to learn &amp; understand.
&gt;
&gt; — [Sorbet user-facing design principles](https://github.com/sorbet/sorbet/#sorbet-user-facing-design-principles)

Another way to read this is, &quot;we let other people blaze trails, and then copy their good
ideas.&quot;

~~This question comes up often enough that it makes me want to imagine that some sort of
similar tool exists for other dynamically typed languages? But as far as I'm aware,
no sort of type-aware linter exists for TypeScript, Flow, or Mypy.[^comp] Not having any
sort of frame of reference makes it hard to gauge expectations people have when asking for
a tool like this.~~

[^comp]: ~~If you know of a comparable tool, please do share!~~

**Update, 2022-05-18**[^steved] There _are_ type-aware static analysis tools for C# and
TypeScript. Both languages were designed by the
[same person](https://en.wikipedia.org/wiki/Anders_Hejlsberg), so maybe this isn't
surprising. Unfortunately for Sorbet, they were architected to support static analysis
tooling
[from the
beginning](https://en.wikipedia.org/wiki/Roslyn_%28compiler%29#:~:text=Roslyn%20was%20designed%20with%20that%20intent%20from%20the%20beginning.).
Sorbet's current architecture was instead designed for batch type checking performance on
large monorepos,[^perf] and IDE support was grafted on later. Exposing hackable APIs has
so far not been considered.

[^steved]:
  Thanks to Steve Dignam for pointing out that not only does C# have static
  analysis APIs, but that TypeScript does as well, along with an ecosystem of type-aware
  lint rules.

[^perf]:
  All things considered, it's actually quite good at this.

For example, TypeScript offers a [compiler API][tsc-api], which is then used by the
TypeScript ESLint project, which allows defining [custom type-aware lint rules]. What can
we learn from this project?

[tsc-api]: https://github.com/microsoft/TypeScript/wiki/Using-the-Compiler-API
[custom type-aware lint rules]: https://typescript-eslint.io/docs/development/custom-rules#type-checking

- The TypeScript compiler API does not have any sort of backwards compatibility guarantee,
  so breaking changes are published from time to time.
- It appears that all TypeScript functionality can be accessed behind the API, including
  instantiating a stateful object representing the type checker, running the type checker
  end-to-end on a project, spawning an LSP server, etc.
- The way custom lint rules are written is by converting between ESLint's AST node type
  and TypeScript's AST node type. The TypeScript compiler APIs then allow asking for the
  type of an AST (i.e., expression). I haven't confirmed, but this leads me to believe
  that TypeScript itself is doing typechecking on the AST (maybe with some auxiliary
  structures to track control flow), not on a CFG like Sorbet, which makes it easier to
  present the kind of API that makes sense in a lint rule.

It's interesting to [browse the rules that require type information][requires-type-info]
to get a sense for what's possible. Things like `strict-boolean-expressions` and
`no-floating-promises` are examples of non-trivial lints using type information.

[requires-type-info]: https://cs.github.com/typescript-eslint/typescript-eslint?q=%22requiresTypeChecking%3A+true%22

I have spent very limited time looking into how things work exactly, so it's possible I'm
misrepresenting the ideas. In any case, I personally still draw the same conclusion:
clearly people in the TypeScript community derive value from building type-aware lint
rules, and TypeScript is well-architected to enable this. As mentioned in previous
sections, Sorbet's current architecture does not present the same conveniences.

### Competing priorities tend to win out

When attempting to build this feature, we'd of course have to judge the cost of what we'd
have to give up.

Overwhelmingly, the requests people have about Sorbet are:

- Please fix shape and tuple types.
- Please fix generics (classes and methods).
- Please make Sorbet work faster on large codebases, especially in IDE mode.
- Please build more refactoring tools. If IntelliJ can do it, I'd like Sorbet to do it too.

So far, these requests have taken priority over greenfield projects, including things like
a type-aware linter.

### &quot;Maybe in the future...&quot;

Those are my current thoughts on the topic. Obviously, a lot of these reasons are just &quot;it's
hard,&quot; and maybe for someone else those things would be easy. Others are just selfish,
&quot;it's convenient for us to not have to think about compatibility,&quot; and so they're easy to
disagree with. Some of them are, &quot;there's no clear answer to this question,&quot; and sometimes
you can wave those away by just picking _any_ answer and living with it, rather than
searching for the best.

So while I don't think that Sorbet would _never_ get some sort of type-aware linter, so
far there are many factors that present a pretty high barrier to building something like
this. Hopefully this post sheds some light on why a type-aware linter for Sorbet does not
currently exist.</content><author><name>Jake Zimmerman</name></author><category term="sorbet" /><category term="ruby" /><category term="rubocop" /><summary type="html">← Return home This post represents my opinions at a point in time. It’s not necessarily the views of my team or my employer.</summary></entry><entry><title type="html">Parse Error Recovery in Sorbet: Part 4</title><link href="http://localhost:4000/error-recovery-part-4/" rel="alternate" type="text/html" title="Parse Error Recovery in Sorbet: Part 4" /><published>2022-04-17T19:27:29-04:00</published><updated>2022-04-17T19:27:29-04:00</updated><id>http://localhost:4000/error-recovery-part-4</id><content type="html" xml:base="http://localhost:4000/error-recovery-part-4/">&lt;nav id=&quot;TOC&quot; role=&quot;doc-toc&quot;&gt;
  &lt;a href=&quot;/&quot;&gt;← Return home&lt;/a&gt;&lt;br&gt;
&lt;/nav&gt;

&lt;main&gt;
&lt;p&gt;This is the fourth post in a series about “things I’ve learned while making improvements to Sorbet’s parser.” With the last post, I talked about some tools and techniques that I’ve found useful while hacking on Sorbet’s &lt;a href=&quot;https://www.gnu.org/software/bison/&quot;&gt;Bison&lt;/a&gt;-based parser. This post is going to continue that theme by explaining in a little more detail the primary tool Bison has for adding error recovery to a parser: the special &lt;code&gt;error&lt;/code&gt; token.&lt;/p&gt;
&lt;p&gt;You don’t &lt;em&gt;really&lt;/em&gt; need to read the previous posts for this post to be useful, but if in case you want to queue them up to read later, here’s the list:&lt;/p&gt;
&lt;!-- more --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;/error-recovery-part-1/&quot;&gt;Part 1: Why Recover from Syntax Errors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/error-recovery-part-2/&quot;&gt;Part 2: What I Didn’t Do&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/error-recovery-part-3/&quot;&gt;Part 3: Tools and Techniques for Debugging a (Bison) Parser&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;/error-recovery-part-4/&quot;&gt;Part 4: Bison’s &lt;code&gt;error&lt;/code&gt; Token&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;(&lt;em&gt;coming soon&lt;/em&gt;) Part 5: Backtracking, aka Lexer Hacks&lt;/li&gt;
&lt;li&gt;(&lt;em&gt;coming soon&lt;/em&gt;) Part 6: Falling Back on Indentation, aka More Lexer Hacks&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That being said, if you’re also trying to hack on a Bison parser to make it recover from errors, I hate to say it but this post is not going to be a substitute for the &lt;a href=&quot;https://www.gnu.org/software/bison/manual/bison.html#Error-Recovery&quot;&gt;official docs on Error Recovery&lt;/a&gt;, so you’re going to want to spend some time skimming that section of the docs if you haven’t already.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Bison doesn’t automatically recover from errors. When specifying a language’s grammar in Bison, it needs explicit annotations in various places to inform how it should recover from errors. This is unlike some parser tools like tree-sitter, which automatically generate the error recovery for for you, but which you don’t have a huge amount of control over. (Don’t let that trick you though, Bison still places substantial limitations on how you can recover from errors. a hand-written recursive descent parser is maximally flexible, though it’s also pretty for it to backfire. bison’s error recovery at least still checks for conflicts automatically (which means checking for ambiguities automatically))&lt;/p&gt;
&lt;p style=&quot;width: 50%; float: left; text-align: left;&quot;&gt;
&lt;a href=&quot;/error-recovery-part-3/&quot;&gt;← Part 3: Tools and Techniques for Debugging a (Bison) Parser&lt;/a&gt;
&lt;/p&gt;
&lt;p style=&quot;width: 50%; float: right; text-align: right;&quot;&gt;
(&lt;em&gt;coming soon&lt;/em&gt;) Part 5: Backtracking, aka Lexer Hacks →
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/main&gt;</content><author><name>Jake Zimmerman</name></author><category term="TODO" /><summary type="html">← Return home This is the fourth post in a series about “things I’ve learned while making improvements to Sorbet’s parser.” With the last post, I talked about some tools and techniques that I’ve found useful while hacking on Sorbet’s Bison-based parser. This post is going to continue that theme by explaining in a little more detail the primary tool Bison has for adding error recovery to a parser: the special error token.</summary></entry><entry><title type="html">T::Enum Pros &amp;amp; Cons</title><link href="http://localhost:4000/tenum-pro-con/" rel="alternate" type="text/html" title="T::Enum Pros &amp;amp; Cons" /><published>2022-03-17T19:13:16-04:00</published><updated>2022-03-17T19:13:16-04:00</updated><id>http://localhost:4000/tenum-pro-con</id><content type="html" xml:base="http://localhost:4000/tenum-pro-con/">One feature that Sorbet doesn't have[^yet][^but-actually] but gets requested frequently
is support for literal string and symbol types. Something like `T.any(:left, :right)`,
which is a type that allows either the symbol literal `:left` or `:right`, but no other
`Symbol`s much less other types of values. The closest that Sorbet has to this right now
is typed enums:

[^yet]:
  Yet. The biggest limitation is just that Sorbet's approach to type inference is designed
  to run fast and be simple to understand, sometimes sacrificing power.

[^but-actually]:
  ... but actually Sorbet already has these types internally 😅 It's just that it doesn't
  have syntax for people to write them in type annotations. And lo, it's [because they're
  buggy], but for the things where Sorbet needs to use them internally we can
  intentionally work around the known bugs, so it hasn't been worth the pain to fix.


```ruby
class LeftOrRight &lt; T::Enum
  enums do
    Left = new
    Right = new
  end
end
```

TypeScript, Flow, and Mypy all have literal types. You probably have felt yourself wanting
this. I don't really have to explain why they're nice. But I'll do it anyways, just to
prove that I hear you.

\

## 👎 `T::Enum` cannot be combined in ad hoc unions.

That's a fancy way of saying we'd like to be able to write `T.any(:left, :right)` in any
type annotation, without first having to pre-declare the new union type to the world. I
spoke at length about how the existence of ad hoc union types make handling exceptional
conditions [more pleasant than checked exceptions][ad-hoc-exceptions], so I'm right there
with you in appreciating that feature.

## 👎 `T::Enum` is verbose.

Even if you wanted to pre-declare the enum type. Consider:

```ruby
LeftOrRight = T.type_alias {T.any(:left, :right)}
```

Boom. One line, no boilerplate. Wouldn't that be nice?


## 👎 It's hard to have one `T::Enum` be a subset of another.

This comes up so frequently that there's [an FAQ entry][subset] about it. The answer is
yet more verbosity and boilerplate.

\

So I hear you. But I wanted to say a few things in defense of `T::Enum`, because I think
that despite how nice it might be to have literal types (and again, we may yet build them
one day), there are still *a lot of points* in favor of `T::Enum` as it exists today.

\

## 🚀 Every IDE feature Sorbet supports works for `T::Enum`.

`T::Enum`s are just normal constants. Sorbet supports finding all constant references,
renaming constants, autocompleting constant names, jumping to a constant's definition,
hovering over a constant to see its documentation comment. Also all of those features work
on both the enum class itself and each individual enum value.

We could _maybe_ support completion for symbol literals in limited circumstances, but it
would be the first of its kind in Sorbet. Same goes for rename, and maybe find all
references. Jump to Definition I guess would want to jump not to the actual definition,
but rather to the signature that specified the literal type? It's weird.

## 🙊 `T::Enum` guards against basically all typos.

Even in `# typed: false` files! Even when calling methods that take don't have signatures,
or that have loose signatures like `Object`! Incidentally, this is basically the same
reason why find all references can work so well.

## 🤝 It requires being intentional.

Code gets out of hand really quickly when people try to cutely interpolate strings into
other strings that hold meaning. I'd much rather deal with this:

```ruby
direction = [left_or_right, up_or_down]
```

than this:

```ruby
direction = &quot;#{left_or_right}__#{up_or_down}&quot;
```

If you try to do this with `T::Enum` you get strings that look like:

```ruby
'#&lt;LeftOrRight::Left&gt;__#&lt;UpOrDown::Up&gt;'
```

which confuses people, so they ask how to do the thing they're trying to do, which is a
perfect opportunity to talk them down from that cliff. If people decide that yes, this
really is the API we need, we can be intentional about it with `.serialize`:

```ruby
direction = &quot;#{left_or_right.serialize}__#{up_or_down.serialize}&quot;
```

## 🕵️ It's easy to search for.

This is a small one, but I'll mention it anyways. It's quick to search the Sorbet docs for
`T::Enum` and get to the right page. It's similarly easy to find examples of it being used
in a given codebase, to learn from real code. There's no unique piece of syntax in
`T.any(:left, :right)` that is a surefire thing to search for.

[because they're buggy]: https://sorbet.run/#%23%20typed%3A%20true%0Ax%20%3D%20%3Adefault%0A%0A1.times%20do%0A%20%20%23%20Sorbet%20does%20not%20report%20an%20error%20here%0A%20%20%23%20%28it%20would%20have%20to%20start%20doing%20so%29%0A%20%20x%20%3D%20%3Afirst%0Aend%0A%0AT.reveal_type%28x%29%20%23%20Sorbet%20shows%20the%20wrong%20type%20here%0A%0A%23%20Sorbet%20can't%20tell%20the%20difference%20bewteen%20a%20hash%20literal%0A%23%20with%20a%20variable%20key%20versus%20with%20a%20symbol%20literal%20key%0A%23%20at%20the%20time%20that%20inference%20happens.%0AT.reveal_type%28%7Bx%20%3D%3E%20nil%7D%29%0AT.reveal_type%28%7B%3Adefault%20%3D%3E%20nil%7D%29

[ad-hoc-exceptions]: /union-types-checked-exceptions/

[subset]: https://sorbet.org/docs/tenum#defining-one-enum-as-a-subset-of-another-enum</content><author><name>Jake Zimmerman</name></author><category term="fragment" /><category term="sorbet" /><summary type="html">← Return home One feature that Sorbet doesn’t have[^yet][^but-actually] but gets requested frequently is support for literal string and symbol types. Something like T.any(:left, :right), which is a type that allows either the symbol literal :left or :right, but no other Symbols much less other types of values. The closest that Sorbet has to this right now is typed enums:</summary></entry><entry><title type="html">Error Recovery Pastebin</title><link href="http://localhost:4000/error-recovery-pastebin/" rel="alternate" type="text/html" title="Error Recovery Pastebin" /><published>2022-02-28T03:03:44-05:00</published><updated>2022-02-28T03:03:44-05:00</updated><id>http://localhost:4000/error-recovery-pastebin</id><content type="html" xml:base="http://localhost:4000/error-recovery-pastebin/">&lt;nav id=&quot;TOC&quot; role=&quot;doc-toc&quot;&gt;
  &lt;a href=&quot;/&quot;&gt;Home&lt;/a&gt;&lt;br&gt;
  &lt;strong&gt;Contents&lt;/strong&gt;&lt;label for=&quot;contents&quot;&gt;⊕&lt;/label&gt;
  &lt;input type=&quot;checkbox&quot; id=&quot;contents&quot;&gt;
  &lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#understand-bisons-error-recovery-algorithm&quot;&gt;Understand Bison’s error recovery algorithm&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#using-the-error-token-well&quot;&gt;Using the &lt;code&gt;error&lt;/code&gt; token well&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#try-actually-reading-the-generated-parser&quot;&gt;Try actually reading the generated parser&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-problem-might-be-the-lexer&quot;&gt;The problem might be the lexer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#take-a-program-that-parses-delete-one-token-make-it-parse&quot;&gt;Take a program that parses, delete one token, make it parse&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;

&lt;main&gt;
&lt;!-- vim:tw=90
--&gt;
&lt;p&gt;Notably, Sorbet is the first project in this lineage of parsers to care excessively about error recovery. Sorbet prizes editor integration highly, and if the parser produces no parse result for an invalid Ruby file, no downstream editor functionality works (like completion, jump to definition, and hover).&lt;/p&gt;
&lt;p&gt;Now let’s dive into the tips.&lt;/p&gt;
&lt;h1 id=&quot;understand-bisons-error-recovery-algorithm&quot;&gt;Understand Bison’s error recovery algorithm&lt;/h1&gt;
&lt;!--
TODO(jez) Also worth noting that if you're using location information, the location of the
error token expands as it decides to throw things away (it's not just the location of the
lookahead token at the time a synatx error was encountered).
--&gt;
&lt;p&gt;There’s a vague description of this algorithm &lt;a href=&quot;https://www.gnu.org/software/bison/manual/bison.html#Error-Recovery&quot;&gt;in the docs&lt;/a&gt;, but I found that I had to make it more explicit before I could use it well. At a high level, this is what Bison does:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Encounter an error (i.e., it doesn’t expected the current lookahead token).&lt;/li&gt;
&lt;li&gt;Report an error by calling the (user-defined) &lt;code&gt;parser::error&lt;/code&gt; function.
&lt;ul&gt;
&lt;li&gt;This function is always called, and called even before attempting to shift the &lt;code&gt;error&lt;/code&gt; token and recover from the error. This can be either a blessing (you know that adding &lt;code&gt;error&lt;/code&gt; to a production rule will never prevent an error from being reported), or a curse (you don’t get an easy way to customize the error message in context once an &lt;code&gt;error&lt;/code&gt; rule is matched).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Leave the lookahead token untouched, and immediately shift the &lt;code&gt;error&lt;/code&gt; token.&lt;/li&gt;
&lt;li&gt;Check whether we can reduce. If we can’t, &lt;em&gt;completely discard&lt;/em&gt; the object on the stack immediately before the error token (i.e., whatever we had most recently shifted or reduced before encountering the syntax error).
&lt;ul&gt;
&lt;li&gt;If we can, reduce, and continue. Remember that the lookahead token will still be set to whatever it was when the error occurred.&lt;/li&gt;
&lt;li&gt;If we can’t, repeat. Keep discarding until we’ve matched a rule that consumed the &lt;code&gt;error&lt;/code&gt; token or discarded everything, then continue reading new tokens.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It’s conceptually very simple, which is convenient, but has a few gotchas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It’s very easy to throw away important stuff. For example, Sorbet has a generic “attempt to recover from anything” rule for &lt;code&gt;stmts&lt;/code&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;stmts: // nothing
        {
          $$ = driver.alloc.node_list();
        }
    | stmt
        {
          $$ = driver.alloc.node_list($1);
        }
    | stmts terms stmt
        {
          $1-&amp;gt;emplace_back($3);
          $$ = $1;
        }
    | error
        {
          $$ = driver.alloc.node_list();
        }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This says that a list of statements is either empty, a single &lt;code&gt;stmt&lt;/code&gt;, a &lt;code&gt;stmt&lt;/code&gt; after a list of &lt;code&gt;stmts&lt;/code&gt; and any number of terminators (&lt;code&gt;:&lt;/code&gt; or &lt;code&gt;\n&lt;/code&gt;), or &lt;strong&gt;any error&lt;/strong&gt;. But consider how that interacts with this program:&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb2&quot;&gt;&lt;pre class=&quot;sourceCode ruby&quot;&gt;&lt;code class=&quot;sourceCode ruby&quot;&gt;&lt;span id=&quot;cb2-1&quot;&gt;&lt;a href=&quot;#cb2-1&quot; aria-hidden=&quot;true&quot; tabindex=&quot;-1&quot;&gt;&lt;/a&gt;&lt;span class=&quot;cf&quot;&gt;def&lt;/span&gt; foo&lt;/span&gt;
&lt;span id=&quot;cb2-2&quot;&gt;&lt;a href=&quot;#cb2-2&quot; aria-hidden=&quot;true&quot; tabindex=&quot;-1&quot;&gt;&lt;/a&gt;  &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;at&quot;&gt;.times&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-3&quot;&gt;&lt;a href=&quot;#cb2-3&quot; aria-hidden=&quot;true&quot; tabindex=&quot;-1&quot;&gt;&lt;/a&gt;&lt;span class=&quot;cf&quot;&gt;end&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;What happens here is that at the point where the error is encountered (the &lt;code&gt;end&lt;/code&gt; token on the third line), it will happily discard &lt;strong&gt;all&lt;/strong&gt; previous objects, even the &lt;code&gt;def foo&lt;/code&gt;, before matching the &lt;code&gt;| error&lt;/code&gt; rule. So even though we have an error recovery rule, in this case, our parse is going to be empty anyways. Which bring us to our next tip.&lt;/p&gt;
&lt;h1 id=&quot;using-the-error-token-well&quot;&gt;Using the &lt;code&gt;error&lt;/code&gt; token well&lt;/h1&gt;
&lt;p&gt;I’ve found these tips for using Bison’s &lt;code&gt;error&lt;/code&gt; token useful, in light of how the error recovery algorithm works:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Whenever possible, add as much preceding context to the production using the &lt;code&gt;error&lt;/code&gt; token. Like in the example above, we added the &lt;code&gt;error&lt;/code&gt; token up in the &lt;code&gt;f_arg:&lt;/code&gt; rule so that we could write the rule like &lt;code&gt;| f_arg tCOMMA error&lt;/code&gt;, instead of adding it to the rule for &lt;code&gt;f_norm_arg&lt;/code&gt; with no preceding context, like &lt;code&gt;| error&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Adding the prefix has given me the best results for recovering from the error on my first attempted grammar edit without having to reason through conflicts, and also means that little to none of the already-parsed program has to be discarded.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Figure out what are the most common edits, and make every prefix of the stack along that edit have an &lt;code&gt;error&lt;/code&gt; production. For example, consider inserting a new keyword arg:&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb3&quot;&gt;&lt;pre class=&quot;sourceCode ruby&quot;&gt;&lt;code class=&quot;sourceCode ruby&quot;&gt;&lt;span id=&quot;cb3-1&quot;&gt;&lt;a href=&quot;#cb3-1&quot; aria-hidden=&quot;true&quot; tabindex=&quot;-1&quot;&gt;&lt;/a&gt;foo(a, &lt;span class=&quot;wa&quot;&gt;y: &lt;/span&gt;y) &lt;span class=&quot;co&quot;&gt;# contents before edit&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb3-2&quot;&gt;&lt;a href=&quot;#cb3-2&quot; aria-hidden=&quot;true&quot; tabindex=&quot;-1&quot;&gt;&lt;/a&gt;foo(a, x &lt;span class=&quot;wa&quot;&gt;y: &lt;/span&gt;y)&lt;/span&gt;
&lt;span id=&quot;cb3-3&quot;&gt;&lt;a href=&quot;#cb3-3&quot; aria-hidden=&quot;true&quot; tabindex=&quot;-1&quot;&gt;&lt;/a&gt;foo(a, &lt;span class=&quot;wa&quot;&gt;x: y: &lt;/span&gt;y)&lt;/span&gt;
&lt;span id=&quot;cb3-4&quot;&gt;&lt;a href=&quot;#cb3-4&quot; aria-hidden=&quot;true&quot; tabindex=&quot;-1&quot;&gt;&lt;/a&gt;foo(a, &lt;span class=&quot;wa&quot;&gt;x: &lt;/span&gt;x &lt;span class=&quot;wa&quot;&gt;y: &lt;/span&gt;y)&lt;/span&gt;
&lt;span id=&quot;cb3-5&quot;&gt;&lt;a href=&quot;#cb3-5&quot; aria-hidden=&quot;true&quot; tabindex=&quot;-1&quot;&gt;&lt;/a&gt;foo(a, &lt;span class=&quot;wa&quot;&gt;x: &lt;/span&gt;x, &lt;span class=&quot;wa&quot;&gt;y: &lt;/span&gt;y) &lt;span class=&quot;co&quot;&gt;# edit finished&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Ideally there’s an &lt;code&gt;error&lt;/code&gt; production for every intermediate state here, because adding a keyword argument to a method call is common. Probably method calls and variable assignments will be the most commonly edited constructs in all languages.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;try-actually-reading-the-generated-parser&quot;&gt;Try actually reading the generated parser&lt;/h1&gt;
&lt;h1 id=&quot;the-problem-might-be-the-lexer&quot;&gt;The problem might be the lexer&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;sorbet’s lexer has states, sometimes will emit a &lt;code&gt;tNL&lt;/code&gt;, sometimes it won’t&lt;/li&gt;
&lt;li&gt;again: use trace diffs to figure out why it won’t take a production rule you&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;take-a-program-that-parses-delete-one-token-make-it-parse&quot;&gt;Take a program that parses, delete one token, make it parse&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;This is very similar to how people make edits (renames, replaces)&lt;/li&gt;
&lt;/ul&gt;
&lt;!--

- Enable traces
- Make the traces good (printers for tokens)
- Read the traces to get a sense for how the error recovery algorithm works
- Look at the generated `txt` file to learn what the states mean
  - idea: show an annotated trace file in the blog post? (what all the numbers mean)
  - will also show you which states have conflicts
  - (rules in `[...]` are the conflicts, the one in `[...]` is not taken)
- Compare successful parses with unsuccessful parses (vimdiff)
- Go to the place where the two diverge, and add an error case
- Almost all the rules you add are going to end with `error`, but some might not
- Make special error nodes/error names
- You can use `@1`, ... to access the location information of the error token (you have to
  change the parser to thread this information through it)
- The error token's location will be set to the location of the lookahead token that
  caused the syntax error to be discovered.
- Rule of thumb (in my experience) for resolving conflicts is that I want as many tokens
  in from of the `error` token in a rule as possible
  - going to throw away less of the program, but also going to make it less likely to
    conflict
  - most of my conflicts come from attempting to put `| error [...]` in two adjacent
    productions.
- Figure out what are the most common edits, and make every incremental edit along that
  path recover. PRs can show you the kinds of edits people show up with, if you need a
  starting point--imagine someone typed out every character of a PR's diff serially.
- Probably method calls and variable assignments are the most important (that's where
  people are most likely to be editing)

- Have some sort of fallback rule
- Know that the fallback rule is going to conflict with some of the things you'll want
  to build.
- Midrule actions are real rules. Just give a name to the midrule, then use that rule in
  the place where you wanted to.
- Can do tricks to get better error messages too
- Note: yyerror is going to be called every time (you can check the generated code to see
  that yyerror is called well before the error token is even shifted)
- Sometimes you might want to change the lexer to make things easier to detect
- Rants about ruby (keyword method names, def/end instead of curly braces)
- Don't be afraid to read the the generated parser's source. One way to do this is to set
  a breakpoint on a line in your parser file. Another way is to grep for your actions'
  code in the generated file.

--&gt;
&lt;/main&gt;</content><author><name>Jake Zimmerman</name></author><summary type="html">← Return home Notably, Sorbet is the first project in this lineage of parsers to care excessively about error recovery. Sorbet prizes editor integration highly, and if the parser produces no parse result for an invalid Ruby file, no downstream editor functionality works (like completion, jump to definition, and hover).</summary></entry><entry><title type="html">Parse Error Recovery in Sorbet: Part 3</title><link href="http://localhost:4000/error-recovery-part-3/" rel="alternate" type="text/html" title="Parse Error Recovery in Sorbet: Part 3" /><published>2022-02-28T00:25:56-05:00</published><updated>2022-02-28T00:25:56-05:00</updated><id>http://localhost:4000/error-recovery-part-3</id><content type="html" xml:base="http://localhost:4000/error-recovery-part-3/">This is the third post in a series about &quot;things I've learned while making improvements to
Sorbet's parser.&quot; Previously I discussed high level questions like why it's an important
problem for Sorbet to solve and some approaches I decided not to take. This post switches
gears to focus on specific tips and techniques I found useful while making parser changes.

&lt;!-- more --&gt;

- [Part 1: Why Recover from Syntax Errors][part1]
- [Part 2: What I Didn't Do][part2]
- **[Part 3: Tools and Techniques for Debugging a (Bison) Parser][part3]**
- [Part 4: Bison's `error` Token][part4]
- (*coming soon*) Part 5: Backtracking, aka Lexer Hacks
- (*coming soon*) Part 6: Falling Back on Indentation, aka More Lexer Hacks

[part1]: /error-recovery-part-1/
[part2]: /error-recovery-part-2/
[part3]: /error-recovery-part-3/
[part4]: /error-recovery-part-4/
[part5]: /error-recovery-part-5/
[part6]: /error-recovery-part-6/

With that all out of the way, let's dive into the tips.

# Read the docs

Haha! You probably thought that by Googling for things you'd be able to find something
that lets you avoid reading the official docs. But it's boring for me to repeat everything
that's in the docs, and honestly the Bison and Ragel docs are rather comprehensive as far
as software documentation goes these days:

→ [Ragel User Guide](https://www.colm.net/files/ragel/ragel-guide-6.9.pdf)\
→ [Bison User Guide](https://www.gnu.org/software/bison/manual/bison.html)

But I will give you some tips for **how** to read the docs:

-   You ~always want the &quot;HTML entirely on one web page&quot; version of the Bison docs—it's
    way easier to ⌘F around one page.

-   Bison actually gets new, interesting features from version to version. Double check
    that the version of the docs you're reading actually match the version of Bison you're
    using. I haven't found an easy way to read old Bison docs online, so I usually just
    `grep` for things in the docs' sources:

    ```
    ❯ git clone https://github.com/akimd/bison
    ❯ git checkout v3.3.2
    ❯ grep -r 'error.*token' doc/
    doc/bison.texi:error.  If there is a @samp{..} token before the next
    doc/bison.texi:value of the error token is 256, unless you explicitly assigned 256
    ... many more results ...
    ```

-   I've found it valuable to actually take my time while reading the Bison docs. I've
    found a lot of things that turned out to be relevant later on because I took the time
    to read parts of the docs that didn't look immediately relevant.

But that's enough soapbox standing, now we return to regularly scheduled tips.

# Enable traces, and make them good

Before I started working on this project, I treated Sorbet's parser like a black box. In
the spirit of &quot;[Computers can be understood],&quot; the first thing I did was enable traces for
our parser. Easy enough:

- Define the [`parse.trace`] variable in the grammar
- Call [`set_debug_level`] on the generated parser

Here's [the PR in Sorbet][4985], which might help to make these two steps more concrete.

The trace output looks something like this:

```
❯ sorbet --trace-parser -e 'def foo; end'
Starting parse
Entering state 0
Reading a token: Next token is token &quot;def&quot; ()
Shifting token &quot;def&quot; ()
Entering state 4
Reading a token: Next token is token tIDENTIFIER ()
Shifting token tIDENTIFIER ()
Entering state 184

...
```

The output is somewhat useful as is, but it can be better. First, all the trailing `()` on
the &quot;Next token is ...&quot; lines are present because there aren't any `%printer`'s for those
tokens--we can easily get the trace to not only show that it read a `tIDENTIFIER` token,
but also what the name of that variable was. After adding one for `tIDENTIFIER` like this:

```
%printer { yyo &lt;&lt; $$-&gt;view(); } tIDENTIFIER
```

The `$$-&gt;view();` bit calls the `view` method on Sorbet's token type, converting it to a string. Now our traces look better:

```
❯ sorbet --trace-parser -e 'def foo; end'
Starting parse
Entering state 0
Reading a token: Next token is token &quot;def&quot; ()
Shifting token &quot;def&quot; ()
Entering state 4
Reading a token: Next token is token tIDENTIFIER (foo)
Shifting token tIDENTIFIER (foo)
Entering state 184
Reading a token: Next token is token &quot;;&quot; ()
Reducing stack by rule 125 (line 1140):
   $1 = token tIDENTIFIER (foo)
-&gt; $$ = nterm fname ()
...
```

So far I've been adding these `%printer`s only as I encounter the tokens that show up,
mostly because I'm too lazy to exhaustively define printers for all the tokens—Ruby has a
lot of tokens. You'll note that Bison even lets you register `%printer`s for non-terminals
(not pictured, but the same mechanism). You could use this to, like, print the currently
reduced AST for that non-terminal, or some other summary.

The next step is to actually understand what these traces mean, because it looks like
there's a lot of magic names and numbers, but there's a short cut for that.

# Diff traces for good and bad parses

This code is a syntax error in Ruby:

```ruby
def foo(x,); end
```

Bison has a fancy `error` token that we can use to recover from cases like this, but it's
hard to know where to add that `error` token into the grammar. Printing the trace file
would likely help us figure out where, but even when we're staring at the trace file it's
not entirely clear.

Luckily there's a short cut:

1.  Record a parser trace for the invalid parse.
2.  &quot;Fix&quot; the file so that it parses by only adding tokens, and record a trace for that
    parse.
    - This ensures that all the tokens present in the bad parse are also present in the
      good parse.
3.  `diff` (or `vimdiff`) the two traces, and add an error recovery rule to the place
    where the trace differs.

In our case, I want `def foo(x,); end` to parse as if the user had properly written two
arguments, so that I can record the fact that the user started to introduce a second
argument. I'll record a trace for the program `def foo(x, y); end`, and diff it. The diff
looks like this:

&lt;figure class=&quot;left-align-caption&quot;&gt;
```{.diff .numberLines .hl-30 .hl-31 .hl-32 .hl-33 .hl-34}
❯ sorbet --trace-parser -e 'def foo(x,); end' 2&gt; trace-bad.txt;
  sorbet --trace-parser -e 'def foo(x, y); end' 2&gt; trace-good.txt;
  diff -u trace-bad.txt trace-good.txt
--- trace-bad.txt       2022-01-16 14:40:51.168977798 -0800
+++ trace-good.txt      2022-01-16 14:40:51.728976581 -0800
@@ -53,45 +53,201 @@
 Next token is token &quot;,&quot; ()
 Shifting token &quot;,&quot; ()
 Entering state 752
+Reading a token: Next token is token tIDENTIFIER ()
+Shifting token tIDENTIFIER ()
+Entering state 541
+Reducing stack by rule 660 (line 3400):
+   $1 = token tIDENTIFIER ()
+-&gt; $$ = nterm f_norm_arg ()
+Stack now 752 562 349 78 0
+Entering state 559
+Reducing stack by rule 661 (line 3408):
+   $1 = nterm f_norm_arg ()
+-&gt; $$ = nterm f_arg_asgn ()
+Stack now 752 562 349 78 0
+Entering state 560
 Reading a token: Next token is token &quot;)&quot; ()
-Error: popping token &quot;,&quot; ()
+Reducing stack by rule 662 (line 3414):
+   $1 = nterm f_arg_asgn ()
+-&gt; $$ = nterm f_arg_item ()
+Stack now 752 562 349 78 0
+Entering state 901
+Reducing stack by rule 665 (line 3428):
+   $1 = nterm f_arg ()
+   $2 = token &quot;,&quot; ()
+   $3 = nterm f_arg_item ()
+-&gt; $$ = nterm f_arg ()
...
```
&lt;figcaption&gt;
  This example shows `diff -u` in the command line, but when I'm looking at these traces I
  almost exclusively use `vimdiff`, because it lets me expand surrounding context, search
  for keywords, etc. And it looks nicer.
&lt;/figcaption&gt;
&lt;/figure&gt;


Looking at the highlighted lines near the bottom, we see that eventually the good parse
was able to reduce `nterm f_arg` by combining `f_arg`, `&quot;,&quot;`, and `f_arg_item`. The trace
tells us that this happened in `rule 665 (line 2428)`. That line number is the actual
source line number in our `*.ypp` grammar file.

All we have to do is go to that line and add an error case, which is pretty easy:

```{.diff .numberLines .hl-11 .hl-12 .hl-13 .hl-14}
           f_arg: f_arg_item
                    {
                      $$ = driver.alloc.node_list($1);
                    }
                | f_arg tCOMMA f_arg_item
                    {
                      auto &amp;list = $1;
                      list-&gt;emplace_back($3);
                      $$ = list;
                    }
+               | f_arg tCOMMA error
+                   {
+                     $$ = $1;
+                   }
```

And now the parser reports the error but continues to recover from the error:

```
❯ sorbet -p parse-tree-whitequark -e 'def foo(x,); end'
s(:def, :foo,
  s(:args,
    s(:arg, :x)), nil)
-e:1: unexpected token &quot;)&quot; https://srb.help/2001
     1 |def foo(x,); end
                  ^
Errors: 1
```

This technique of comparing the trace for &quot;what it currently does&quot; against &quot;what I wish
it did&quot; has been super useful, because it often shows exactly the point where the trace
diverged, along with the reason. In this example, the `f_arg_item` was never reduced, but
sometimes the difference will be something like &quot;the lexer didn't read a token&quot; or &quot;the
lexer read a token, but because of the state the lexer was in, it was the wrong token.&quot;
Whatever the cause, comparing traces usually shows the problem.

This particular example also showed an example of using Bison's `error` token. I'll talk
more about what this `error` token means in the next post.

&lt;p style=&quot;width: 50%; float: left; text-align: left;&quot;&gt;
  [← Part 2: Why Recover from Syntax Errors][part2]
&lt;/p&gt;
&lt;p style=&quot;width: 50%; float: right; text-align: right;&quot;&gt;
  [Part 4: Bison's `error` Token →][part4]
&lt;/p&gt;

&lt;br&gt;


[Computers can be understood]: https://blog.nelhage.com/post/computers-can-be-understood/
[`parse.trace`]: https://www.gnu.org/software/bison/manual/bison.html#Tracing
[`set_debug_level`]: https://www.gnu.org/software/bison/manual/bison.html#index-set_005fdebug_005flevel-on-parser
[4985]: https://github.com/sorbet/sorbet/pull/4985/files?w=1#diff-63fada7036ffcba42e6615c3b85615cb81d47aafbf88122a552a34fb799c06b5R17</content><author><name>Jake Zimmerman</name></author><category term="sorbet" /><category term="parsing" /><summary type="html">← Return home This is the third post in a series about “things I’ve learned while making improvements to Sorbet’s parser.” Previously I discussed high level questions like why it’s an important problem for Sorbet to solve and some approaches I decided not to take. This post switches gears to focus on specific tips and techniques I found useful while making parser changes.</summary></entry><entry><title type="html">Parse Error Recovery in Sorbet: Part 2</title><link href="http://localhost:4000/error-recovery-part-2/" rel="alternate" type="text/html" title="Parse Error Recovery in Sorbet: Part 2" /><published>2022-02-22T03:56:34-05:00</published><updated>2022-02-22T03:56:34-05:00</updated><id>http://localhost:4000/error-recovery-part-2</id><content type="html" xml:base="http://localhost:4000/error-recovery-part-2/">This is the second post in a series about &quot;things I've learned while making improvements
to Sorbet's parser.&quot; Specifically, it's about approaches I considered but decided against.

&lt;!-- more --&gt;

- [Part 1: Why Recover from Syntax Errors][part1]
- **[Part 2: What I Didn't Do][part2]**
- [Part 3: Tools and Techniques for Debugging a (Bison) Parser][part3]
- [Part 4: Bison's `error` Token][part4]
- (*coming soon*) Part 5: Backtracking, aka Lexer Hacks
- (*coming soon*) Part 6: Falling Back on Indentation, aka More Lexer Hacks

[part1]: /error-recovery-part-1/
[part2]: /error-recovery-part-2/
[part3]: /error-recovery-part-3/
[part4]: /error-recovery-part-4/
[part5]: /error-recovery-part-5/
[part6]: /error-recovery-part-6/

Before we get started, I should say: I'm not, like, an expert at writing parsers. In fact
of all the changes I've made to Sorbet, it's definitely up there for &quot;changes I've been
least qualified to have made.&quot; But at the end of the day my test cases passed
:upside_down_face: Take my experiences with as many or as few grains of salt as you'd
like. This also means that if you want to suggest other alternatives or otherwise teach me
something new, I'm all ears!

First, a little bit of history. Sorbet's parser was originally a part of the [TypedRuby]
project.[^typedruby]  In turn, TypedRuby sourced its parser by porting the grammar file in
the [whitequark parser] from [Racc] (a Yacc-like parser generator for Ruby) to [Bison] (a
Yacc-like parser generator for C/C++). Sorbet imported the source code of the TypedRuby
parser and continued to modify it over time as Ruby syntax evolved. The lexer uses [Ragel]
(also inherited from whitequark by way of TypedRuby) and tends to be quite stateful
compared to other lexers I've seen—a point which we'll come back to in future posts.

[^typedruby]:
  [TypedRuby] was an aspirational Ruby type checker implemented in Rust that predated
  Sorbet. It is now abandoned.

Importantly...

- Sorbet's parser does not use [Ripper], the parser built into the Ruby VM itself.

  Ripper is meant to be used as a library from Ruby code, not from C++ like Sorbet needs
  for performance.

  Okay technically that's a lie. The [rubyfmt] project manages to depend on Ripper from
  Rust by exposing it via Ruby's support for native (C) extensions. **But** doing that
  comes with [significant build complexity][configure-make], because it has the effect of
  basically importing Ruby's whole `configure &amp;&amp; make` build step.

  Meanwhile it was super easy to import the TypedRuby parser as a self-contained unit with
  basically no questions asked (and remember: Sorbet predates rubyfmt). It's also nice to
  be free from upstream[^upstream] constraints: I can mess around in Sorbet's parser as
  much as I want and the only people I have to defend my choices to are my teammates, not
  the Ruby maintainers.

- Sorbet's parser does not use [tree-sitter].

  Tree-sitter is tool whose main goals are basically 100% aligned with Sorbet's needs in a
  parser: fast enough to run on every keystoke, robust enough to handle syntax errors, and
  native-code friendly. It would seem like a no-brainer for Sorbet to use.

  Unfortunately when I looked closely, it didn't actually pan out. I used the [tree-sitter
  playground] to test a bunch of syntax errors where I wanted to be able to respond to
  completion requests for to see what the parse result looked like. In some cases it
  worked okay, but for the cases I cared about the most (mostly those involving `x.`), the
  results weren't good enough. If I was going to have to manually hack on a parser to get
  it to do what I wanted, I figured I'd rather just stick with what Sorbet already had.

  On top of that tree-sitter is still pretty young, and almost everyone who is using
  tree-sitter right now is using it for two use cases: syntax highlighting, and code
  navigation. If the parse result generates the wrong thing (imagine there's a bug in the
  grammar file that no one else has reported yet), oh well, maybe the colors are wrong or
  the jump-to-def goes to the wrong place. In Sorbet, it would mean either reporting an
  error when there isn't one, or not reporting an error when there is one, both of which
  are particularly bad.

  Given that it was both (1) going to take extra hacks[^hacks] to get working instead of
  being a drop-in solution and (2) potentially trade Sorbet's mature parser for a
  less-mature parser, it didn't seem worth pursuing.

- Sorbet's parser is not hand-written with [recursive descent].

  Many people whose opinions I respect have told me that there's a reason why so many
  people hand-write their parsers: error recovery is easier when given the flexibility to
  bend the whole parser to your will.

  But there isn't an existing hand-written Ruby parser I could start from, and I didn't
  want to completely stall progress with a bug-for-bug rewrite when I already had some
  ideas for how to make the existing parser better. Basically this approach has the same
  tradeoffs as adopting tree-sitter (lots of work with too many unknowns).

[^upstream]:
  I should note that I'm not opposed to upstreaming the changes I've made to Sorbet's
  parser. Some of them intentionally break Ruby compatibility (in minor ways), and
  even the changes that don't would likely require effort to get them merged properly. If
  you find my changes and want to submit them upstream, please go ahead!

[^hacks]:
  After publishing this post, I wrote more about my [thoughts on tree-sitter].

All of these claims about Sorbet's parser were true when I started, and they haven't
changed. You'll notice that in most cases the justification is &quot;I don't have time to do
X&quot; and not &quot;doing X is wrong.&quot; My biggest constraint in improving the parser has been
making small, fast, iterative improvements. I wanted to be left with something to show
even if I had to stop working on the parser sooner than expected. It's possible that
someone with more time or more patience will want to revisit one of these approaches in
the future, and if you do I'd love to hear about it!

Anyways, that rules out the most common refrains from onlookers. But there was another,
more unconventional approach I considered and decided against: using [dead_end].
`dead_end` isn't a Ruby parser but rather a tool that hijacks Ruby's syntax error
reporting mechanism[^hijack] to improve the message for certain syntax errors.
Specifically, it'll try to show error messages in cases like this:

[^hijack]:
  It turns out, all (&quot;all&quot;) you have to do is is monkey patch `require` to `rescue
  SyntaxError`. Thanks Ruby :slightly_smiling_face:

```{.ruby .numberLines .hl-4 .hl-8}
class A
  def foo
    # ... lots of code ...
  # ← dead_end error: missing `end` keyword

  def bar
  end
end # ← ruby default error: unexpected token &quot;end of file&quot;
```

Missing an `end` keyword is a super common class of Ruby syntax errors,[^curly] and
`dead_end` already works particularly well at reporting them, so it was tempting to
~~steal~~ reuse either the code or the ideas.

[^curly]:
  One of my biggest Ruby syntax gripes is that it isn't a curly brace language like C or
  JavaScript. Any sensibly editor will **immediately** insert the matching `}` after first
  typing `{`. But most Ruby editors will only insert the `end` matching some statement
  after a full line has been typed and `&lt;Enter&gt;` has been pressed, if anything. This means
  that unclosed `if`/`while`/`do`/`def`/`class` statements are **abundantly** common in
  Ruby, and this class of error (mismatched pairs) is trickier than the average error.

Early on I had decided not to use the code directly (it's written in Ruby, and I didn't
want to add a runtime dependency on Ruby to Sorbet). But in the end, I decided not to use
its recovery algorithm either.

The algorithm is [described in more detail here][dead_end-algo], but the tl;dr is that it
uses indentation to search for mismatched snippets, expanding and discarding lines from
the search frontier when it finds portions of a Ruby file that properly parse at a given
indentation level.

The problem with taking that idea verbatim is that the end result is basically just a set
of lines in the source file that contain the error. But knowing those lines, there's still
no parse result for those lines. For example:

```{.ruby .numberLines .hl-3}
def foo
  # ... code before ...
  if arbitrary_expression().
  # ... code after ...
end
```

`dead_end` could point to line 3 as the problem, but then I'd still have to parse that
line to be able to e.g. service a completion request after the `.`, which is _basically_
the situation we started with, because the parser would still be on the hook for the full
complexity of what that `arbitrary_expression()` could represent. So I put the `dead_end`
algorithm itself aside as well.

**But!** the general idea of using indentation to guide recovery proved out to be pretty
useful—most Ruby editors will auto-indent and -dedent correctly for most edits—and there
was another way to take advantage of it in Sorbet's parser, along with some other tricks.
The next few posts will discuss those tricks!

&lt;p style=&quot;width: 50%; float: left; text-align: left;&quot;&gt;
  [← Part 1: Why Recover from Syntax Errors][part1]
&lt;/p&gt;
&lt;p style=&quot;width: 50%; float: right; text-align: right;&quot;&gt;
  [Part 3: Tools and Techniques for Debugging a (Bison) Parser →][part3]
&lt;/p&gt;

&lt;br&gt;


[TypedRuby]: https://github.com/typedruby/typedruby
[whitequark parser]: https://github.com/whitequark/parser

[Racc]: https://rubygems.org/gems/racc
[Bison]: https://www.gnu.org/software/bison/
[Ragel]: http://www.colm.net/open-source/ragel/

[Ripper]: https://ruby-doc.org/stdlib-2.7.3/libdoc/ripper/rdoc/Ripper.html
[rubyfmt]: https://github.com/penelopezone/rubyfmt
[configure-make]: https://github.com/penelopezone/rubyfmt/blob/trunk/librubyfmt/build.rs

[tree-sitter]: https://tree-sitter.github.io/tree-sitter/
[tree-sitter playground]: https://tree-sitter.github.io/tree-sitter/playground
[thoughts on tree-sitter]: /tree-sitter-limitations/

[recursive descent]: https://en.wikipedia.org/wiki/Recursive_descent_parser

[dead_end]: https://github.com/zombocom/dead_end
[dead_end-algo]: https://schneems.com/2020/12/01/squash-unexpectedend-errors-with-syntaxsearch/</content><author><name>Jake Zimmerman</name></author><category term="sorbet" /><category term="parsing" /><category term="tree-sitter" /><summary type="html">← Return home This is the second post in a series about “things I’ve learned while making improvements to Sorbet’s parser.” Specifically, it’s about approaches I considered but decided against.</summary></entry><entry><title type="html">Parse Error Recovery in Sorbet: Part 1</title><link href="http://localhost:4000/error-recovery-part-1/" rel="alternate" type="text/html" title="Parse Error Recovery in Sorbet: Part 1" /><published>2022-02-21T03:56:34-05:00</published><updated>2022-02-21T03:56:34-05:00</updated><id>http://localhost:4000/error-recovery-part-1</id><content type="html" xml:base="http://localhost:4000/error-recovery-part-1/">I've spent a lot of time recently making [Sorbet]'s parser recover from syntax errors when
parsing. I didn't have any experience with this before getting started, no one told me
what the good tools or techniques for improving a parser were, and none of the things I
read quite described the ideas I ended up implementing. I figured I'd share the experience
so that you can learn too.

[Sorbet]: https://sorbet.org

&lt;!-- more --&gt;

The original post kept growing and growing as I wrote it, so I broke it up into a handful
of parts:

- [**Part 1: Why Recover from Syntax Errors**][part1]
- [Part 2: What I Didn't Do][part2]
- [Part 3: Tools and Techniques for Debugging a (Bison) Parser][part3]
- [Part 4: Bison's `error` Token][part4]
- (*coming soon*) Part 5: Backtracking, aka Lexer Hacks
- (*coming soon*) Part 6: Falling Back on Indentation, aka More Lexer Hacks

[part1]: /error-recovery-part-1/
[part2]: /error-recovery-part-2/
[part3]: /error-recovery-part-3/
[part4]: /error-recovery-part-4/
[part5]: /error-recovery-part-5/
[part6]: /error-recovery-part-6/

&lt;!-- more --&gt;

This part is going to set the stage a bit and briefly mention why Sorbet cares so much
about syntax errors. The short answer? Editor support is everything.

There are people out there who clamor for a type checker in any codebase they work for.
They're zealous, early-adopters who evangelize types to everyone around them. They love
even just being able to run the type checker in the command line or in CI hand have it
reject code where the types don't check. Sorbet has snuck its way into many codebases this
way! But this approach always introduces friction: there's always a group of people who
see the type checker as an antagonist, sitting there and rejecting code that passes the
test suite and gets the job done.

Having a powerful editor integration drives organic adoption. A command line interface to
a type checker is only really good at reporting errors, but an editor interface exposes so
much more: inline hover lets programmers explore a code's types and documentation by
pointing. Language-aware jump-to-definition and find-all-references mean spending less
time fumbling around a code base and more time looking at the code that's relevant in the
moment. And of course autocompletion is huge. Maybe you're a curmudgeon like me who
doesn't use completion except the occasional keyword completion in Vim, but I've learned
that many, many people feel like moving back to the dark ages when they have to work in a
codebase that doesn't have fast, accurate autocompletion. Every additional editor feature
is another spoonful of sugar—once there are enough, it overwhelms any feeling that the
type checker tastes like medicine.

But if a syntax error means that the parser returns an empty parse result, all those
spoonfuls fall to the floor with a loud clang. Hover and go-to-def are serve stale
(read: imperfect) results at best, if anything. Autocomplete yields no results no matter
how long you wait for the menu to appear.

And in Sorbet's situation, it's even more severe because of how it has chosen to implement
the persistent editor mode. I'm sure I'll discuss this in more depth at some point
(because despite the criticism I'm going to leverage against it, I still think it works
**really** well), but here's a quick overview of Sorbet's language server architecture:

- Nearly every part of Sorbet's offline pipeline is embarrassingly parallel.

  All of the syntactic transformations on the tree happen without access to any sort of
  codebase-wide information. Type inference is only local—inferring types in one method
  body never affects the type check result of another method, let alone another file.
  Program-wide state is made immutable and shared across threads using shared memory (no
  copying).

- Sorbet does not track dependencies.

  That means it doesn't track which files `require` what other files. It doesn't have a
  way to incrementally update its class hierarchy (symbol table) when something changes.
  It only caches parse results and which what errors came from which files. There are no
  module or package boundaries—Sorbet views a codebase as one codebase.[^packager]

- Given all of this, there are two paths in server mode: the fast path and the slow path.

  When an edit comes in, Sorbet quickly decides whether the edit changes any global
  information. If it can, Sorbet throws everything away (except for cached parse results)
  and type checks the entire codebase from scratch. Otherwise, it leaves the symbol table
  unchanged and just retypechecks the edited file.

[^packager]:
  This is starting to change, but only because the approach mentioned here doesn't scale
  to 10 million-line Ruby codebases. It's probably possible to count all such codebases on
  your fingers.

On one hand, this is a very elegant architecture. Sorbet can be almost entirely understood
by how it behaves in batch mode. Put another way, if a user reports a bug in the editor
mode, it almost always reproduces outside of the editor mode. It's rare in Sorbet to find
a bug that only reproduces when the user makes one edit followed by another edit.

But on the other hand, if the parser can't recover from a syntax error, not only can
Sorbet not provide those fancy editor features, it also makes it look like all the
definitions in a file were deleted, which makes it look like the contents of the symbol
table will have changed, which kicks off a retypecheck of the whole codebase. Most syntax
errors are introduced in completely benign places (like `x.` or `if x`), not as part of
changing what's defined in a file (like `def foo` or `X =`) because people spend more time
editing method bodies than anything else. So most syntax errors can take the fast path as
long as the parser can manage to return a decent result.

**All of this is to say**: it's important for Sorbet to recover from syntax errors for two
reasons: it can't provide editor features like completion consistently without it, and in
large codebases it makes Sorbet deliver in-editor type checking errors far faster. In
future posts we'll ramp up to more technical and esoteric parsing topics. In particular,
the next post gives some historical context about Sorbet's parser and some ideas I
rejected for how to get better parse results for syntax errors.

&lt;p style=&quot;text-align: right;&quot;&gt;
  [Part 2: What I Didn't Do →](/error-recovery-part-2/)
&lt;/p&gt;</content><author><name>Jake Zimmerman</name></author><category term="sorbet" /><category term="parsing" /><summary type="html">← Return home I’ve spent a lot of time recently making Sorbet’s parser recover from syntax errors when parsing. I didn’t have any experience with this before getting started, no one told me what the good tools or techniques for improving a parser were, and none of the things I read quite described the ideas I ended up implementing. I figured I’d share the experience so that you can learn too.</summary></entry><entry><title type="html">Sorbet, Generics, and Parametricity</title><link href="http://localhost:4000/sorbet-parametricity/" rel="alternate" type="text/html" title="Sorbet, Generics, and Parametricity" /><published>2022-02-18T02:59:55-05:00</published><updated>2022-02-18T02:59:55-05:00</updated><id>http://localhost:4000/sorbet-parametricity</id><content type="html" xml:base="http://localhost:4000/sorbet-parametricity/">Consider this snippet of Ruby code using Sorbet:

&lt;!-- more --&gt;

&lt;figure class=&quot;left-align-caption&quot;&gt;

```{.ruby .numberLines .hl-11}
# typed: true
extend T::Sig

sig do
  type_parameters(:U)
    .params(x: T.type_parameter(:U))
    .returns(T.type_parameter(:U))
end
def fake_identity_function(x)
  case x
  when Integer then return 0
  #                 ^^^^^^^^ error
  else return x
  end
end
```

&lt;figcaption&gt;
  [→ View on sorbet.run](https://sorbet.run/#%23%20typed%3A%20true%0Aextend%20T%3A%3ASig%0A%0Asig%20do%0A%20%20type_parameters%28%3AU%29%0A%20%20%20%20.params%28x%3A%20T.type_parameter%28%3AU%29%29%0A%20%20%20%20.returns%28T.type_parameter%28%3AU%29%29%0Aend%0Adef%20fake_identity_function%28x%29%0A%20%20case%20x%0A%20%20when%20Integer%20then%20return%200%0A%20%20else%20return%20x%0A%20%20end%0Aend)
&lt;/figcaption&gt;

&lt;/figure&gt;

It has the same signature as the identity function (which returns its argument unchanged),
but doesn't actually do that in all cases. In particular, on the highlighted line it
checks the type of `x` at runtime, and if it's an `Integer`, it always returns `0`,
regardless of the input.

Sorbet flags this as an error (see the full error message in the sorbet.run link).
Sometimes I get asked: &quot;why?&quot; The reasoning for why people think this *shouldn't* be an
error usually looks like this: the signature just says that the output has to be the same
as the input, and `Integer` is the same as `Integer`.

But the fun thing is that this signature makes a stronger constraint on the implementation
of the method—in this case the signature **mandates** that the result is the input. The
hand-wavy intuition for how to think about what's going on is to mentally read the
`type_parameters(:U)` in the signature as &quot;for all,&quot; specifically, &quot;the behavior of this
function is the same *for all* choices of the type parameters.&quot;

In that light, generics put a pretty hefty constraint on the implementation of a generic
method—which is actually a good thing! It means that the caller of the method can make
stronger guarantees about what the method can or cannot do, even seeing only the types. For example:

```ruby
sig do
  type_parameters(:U, :V)
    .params(x: T.type_parameter(:U), y: T.type_parameter(:V))
    .returns(T.any(T.type_parameter(:U), T.type_parameter(:V)))
end
```

From this signature we're guaranteed that the method has to return exactly one of the
arguments we provided (`x` or `y`) and nothing else. It can't invent some third value and
return that.

But the constraints come within reason: the types don't say anything about what side
effects the function might have. This isn't particularly unique to generics (Sorbet
doesn't track side effects no matter the types), but it is worth noting as a sneaky way
that methods can do different things with different arguments. Going back to our
`fake_identity_function` example from earlier:

&lt;figure class=&quot;left-align-caption&quot;&gt;

```{.ruby .numberLines .hl-12 .hl-15}
# typed: true
extend T::Sig

sig do
  type_parameters(:U)
    .params(x: T.type_parameter(:U))
    .returns(T.type_parameter(:U))
end
def fake_identity_function(x)
  case x
  when Integer
    puts(x.even?)
    x
  else
    x.even? # error: Method `even?` does not exist
    x
  end
end
```

&lt;figcaption&gt;
  [→ View on sorbet.run](https://sorbet.run/#%23%20typed%3A%20true%0Aextend%20T%3A%3ASig%0A%0Asig%20do%0A%20%20type_parameters%28%3AU%29%0A%20%20%20%20.params%28x%3A%20T.type_parameter%28%3AU%29%29%0A%20%20%20%20.returns%28T.type_parameter%28%3AU%29%29%0Aend%0Adef%20fake_identity_function%28x%29%0A%20%20case%20x%0A%20%20when%20Integer%0A%20%20%20%20x.even%3F%0A%20%20%20%20x%0A%20%20else%0A%20%20%20%20x.even%3F%0A%20%20%20%20x%0A%20%20end%0Aend)
&lt;/figcaption&gt;

&lt;/figure&gt;

In this example, the side effect of calling `puts(x.even?)` only happens if the type is
`Integer`, breaking the intuition that the behavior of this function is uniform for all
input types.

If Sorbet wanted,[^1] it could prevent this particular form of anti-uniformity by not
allowing any [control-flow-sensitive] type updates. But it wouldn't change the fact that,
for example, one implementation of `fake_identity_function` could always print one log
line, while another implementation could always print two log lines. The only uniformity
guarantees we get are about specifically what's captured in the input and output types.

[^1]:
  Unlike everything we've discussed so far, I'm not actually sure whether that was a
  conscious decision or an accident. But it is a pretty useful feature in practice.

[control-flow-sensitive]: &lt;https://sorbet.org/docs/flow-sensitive&gt;

It turns out that there's a name for this property of generic functions: [parametricity].
It's a fancy word but it basically means what we've talked about here: the implementation
of generic functions are constrained to basically only do one thing, modulo side-effects.
It goes further than just intuition though, and people have done interesting work to
formalize the intuitions into proofs.

[parametricity]: &lt;https://en.wikipedia.org/wiki/Parametricity&gt;</content><author><name>Jake Zimmerman</name></author><category term="ruby" /><category term="sorbet" /><category term="plt" /><summary type="html">← Return home Consider this snippet of Ruby code using Sorbet:</summary></entry></feed>