<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: plt | Bits, Bytes, and Words]]></title>
  <link href="https://blog.jez.io/categories/plt/atom.xml" rel="self"/>
  <link href="https://blog.jez.io/"/>
  <updated>2020-01-02T23:55:47-05:00</updated>
  <id>https://blog.jez.io/</id>
  <author>
    <name><![CDATA[Jake Zimmerman]]></name>
    <email><![CDATA[jake@zimmerman.io]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Notes on Continuations]]></title>
    <link href="https://blog.jez.io/continuations-notes/"/>
    <updated>2019-06-18T18:03:13-07:00</updated>
    <id>https://blog.jez.io/continuations-notes</id>
    <content type="html"><![CDATA[<p>These are some notes I gave out at one of my weekly recitations when I
was teaching <a href="http://www.cs.cmu.edu/~rwh/courses/ppl/">15-312 Principles of Programming Languages</a> at CMU in
April 2017. Continuations have a <em>fascinating</em> analogy with proofs by
contradiction that I might flesh out into a proper post in the future,
but for now here are some rough recitation notes.</p>

<p>They&rsquo;re best understood with Chapter 30 of <a href="http://www.cs.cmu.edu/~rwh/pfpl/">Practical Foundations for
Programming Languages</a> open. (Unfortunately this chapter isn&rsquo;t
available in the online preview of the 2nd edition. I&rsquo;m happy to lend
you my hard copy if I know you IRL.)</p>

<h3>→ <a href="/notes/continuations.pdf">Continuations</a></h3>

<p><strong>Abstract</strong>:</p>

<blockquote><p>Continuations allow for lots of things. Intuitively, we can think of
continuations as &ldquo;functions that never come back.&rdquo; That is,
continuations transfer control to some other part of your program. In
a way, continuations are like a much nicer version of <code>goto</code>. But
they&rsquo;re way more than this—specifically, they reify the concept of a
&ldquo;proof by contradiction&rdquo; into the type system.</p></blockquote>

<!-- vim:tw=72
-->

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ABTs in Haskell]]></title>
    <link href="https://blog.jez.io/abts-in-haskell/"/>
    <updated>2017-11-11T22:31:41-08:00</updated>
    <id>https://blog.jez.io/abts-in-haskell</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve been learning and using Haskell on-and-off for the past couple
of years. One of my early complaints was that I couldn&rsquo;t find a good
library for working with variables and binding that used locally
nameless terms. Recently though, I found <a href="https://github.com/lambdageek/unbound-generics"><code>unbound-generics</code></a>, which
checks all my previously unfilled boxes.</p>

<p>Abstract binding trees (or ABTs) are abstract syntax trees (ASTs)
augmented with the ability to capture the binding structure of a
program. ABTs are one of the first topics we cover in <a href="https://www.cs.cmu.edu/~rwh/courses/ppl/">15-312 Principles
of Programming Languages</a> because variables show up in every
interesting feature of a programming language.</p>

<p>I recently wrote at length about the various strategies for dealing with
<a href="/variables-and-binding/">variables and binding</a> and their implementations. While it&rsquo;s a good
exercise<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> to implement ABTs from scratch, in most cases I&rsquo;d
rather just use a library. In school we used <a href="https://github.com/robsimmons/abbot"><code>abbot</code></a>, which is an ABT
library for Standard ML. For tinkering with Haskell, I recently found
<a href="https://github.com/lambdageek/unbound-generics"><code>unbound-generics</code></a>, which provides a similar API.</p>

<p>I gave it a test drive while learning how to implement type inference
for the simply-typed lambda calculus (STLC) and was rather pleased. The
source code for my STLC inference program is <a href="https://github.com/jez/stlc-infer">on GitHub</a> if
you&rsquo;re looking for an example of <a href="https://github.com/lambdageek/unbound-generics"><code>unbound-generics</code></a> in action.</p>

<p>To pluck a few snippets out, here&rsquo;s the definition of STLC terms:</p>

<pre><code class="haskell">data Term
  = Tvar Tvar
  | Tlam (Bind Tvar Term)
  | Tapp Term Term
  | Tlet Term (Bind Tvar Term)
  | Tz
  | Ts Term
  | Tifz Term Term (Bind Tvar Term)
  | Tbool Bool
  | Tif Term Term Term
  deriving (Show, Generic, Typeable)
</code></pre>

<p><code>Bind</code> is the abstract type for locally nameless terms that bind a
variable. It&rsquo;s cool in Haskell (compared to SML) because the compiler
can automatically derive the locally nameless representation from this
data type definition (with help from the <code>unbound-generics</code> library).</p>

<p>Here&rsquo;s what it looks like in use:</p>

<pre><code class="haskell">-- (This is a snippet from the type inference code)
constraintsWithCon ctx (Tlam bnd) = do
  -- 'out' the ABT to get a fresh variable
  -- (x used to be "locally nameless", but now has a globally unique name)
  (x, e) &lt;- unbind bnd
  -- Generate fresh type variable to put into the context
  t1 &lt;- Cvar &lt;$&gt; fresh (string2name "t1_")
  let ctx' = Map.insert x t1 ctx
  t2 &lt;- constraintsWithCon ctx' e
  return $ Carrow t1 t2
</code></pre>

<p>Apart from <code>out</code> being called <code>unbind</code> and <code>into</code> being called <code>bind</code>,
the API is pretty similar. Also, unlike <code>abbot</code>, which required a
standalone build step to generate SML code, <code>unbound-generics</code> uses the
Haskell&rsquo;s <code>derive Generic</code> to bake the code generation for capture
avoiding substitution and alpha equivalence right into the compiler. All
in all, <code>unbound-generics</code> is really pleasant to use!</p>

<!-- vim:tw=72
-->

<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>In fact, it&rsquo;s hw1 for 15-312! If you&rsquo;re curious, check out the <a href="https://www.cs.cmu.edu/~rwh/courses/ppl/hws/assn1.pdf">handout</a>.<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Variables and Binding]]></title>
    <link href="https://blog.jez.io/variables-and-binding/"/>
    <updated>2017-10-28T19:04:01-07:00</updated>
    <id>https://blog.jez.io/variables-and-binding</id>
    <content type="html"><![CDATA[<p>Variables are central<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> to programming languages, yet they&rsquo;re
often overlooked. Academic PL theory papers usually take for granted
having proper implementations of variables. Most popular languages
butcher variables, <a href="https://existentialtype.wordpress.com/2012/02/01/words-matter/">confusing them with assignables</a>.
Despite being taken for granted, implementing substitution on variables
is easy to get wrong.</p>

<!-- more -->


<p>There are a number of different solutions for handling variables and
binding within a programming language implementation. We&rsquo;ll take a look
at these three:</p>

<ul>
<li>explicit variables,</li>
<li>de Bruijn indices, and</li>
<li>locally nameless terms</li>
</ul>


<p>Before we get to solutions, we need to outline the problem. Implementing
variables and binding reduces to implementing substitution (because
variables are giving meaning by substitution!), and the trickiest part of
substitution is variable capture.</p>

<h2>Variable Capture</h2>

<p>The most common way to get variables and binding wrong is to
accidentally let variables be <em>captured</em> during substitution. Consider
this example:</p>

<pre><code class="python">                      ◀────────────────┐
                             ┌──────┐  │
                       (λx. λy. x + y) y
                         └──────┘
</code></pre>

<p>There are two distinct <code>y</code> variables here:</p>

<ul>
<li>one which refers to the variable bound by the nested lambda</li>
<li>one which refers to some <code>y</code> in the surrounding scope</li>
</ul>


<p>For this example, let&rsquo;s say we choose to represent variables as string
identifiers. If we step the function application, it steps to a
substitution of <code>y</code> for <code>x</code>:</p>

<pre><code class="python">  (λx. λy. x + y) y
  # Apply the function, giving us:
→ [y / x] (λy. x + y)
  # Traverse under the lambda:
→ λy. [y / x] (x + y)
  # Distribute:
→ λy. ([y / x] x) + ([y / x] y)
  # Substitute y where we found an x:
→ λy. y + y
</code></pre>

<p><strong>Note</strong>: <code>[e₁ / x] e₂</code> is read as &ldquo;substitute <code>e₁</code> for <code>x</code> in <code>e₂</code>.&rdquo;</p>

<p>We started with a function which would take two numbers and sum them.
After partially applying that function, we&rsquo;ve ended up with a function
that doubles it&rsquo;s argument. Whoops! We can look at the issue visually in
this diagram:</p>

<pre><code class="python">           ◀─────┐             ┃         ┌──┐
             λy. y + y         ┃        λy. y + y
              └──────┘         ┃         └──────┘
</code></pre>

<p>We were expecting to get out the binding structure on the left, but
instead we got the binding structure on the right. This is called
&ldquo;variable capture&rdquo; or just <strong>capture</strong> for short. The <code>y</code> that we
applied to the summing function was captured by the binding site of the
nested lambda.</p>

<h2>Explicit Variables</h2>

<p>When we&rsquo;re implementing substitution (whether for terms, for types, or
for any other sort of syntax), our primary goal is to implement
<strong>capture-avoiding substitution</strong>. There are many internal
representations we can pick from to achieve this. The strategy above
where variables were simple strings is called <strong>explicit variables</strong>.</p>

<p>Explicit variables are nice because we can represent them directly with
an algebraic data type. For example, for the lambda calculus we might
have this:</p>

<pre><code class="sml">datatype term
  = Var of string
  | Lam of string * term
  | App of term * term
</code></pre>

<p>Implementing capture-avoiding substitution using this representation
isn&rsquo;t pleasant, but it is possible. It uses the observation that there&rsquo;s
no difference between, say <code>λx. x</code> and <code>λy. y</code>. Our choice of variable
names doesn&rsquo;t matter&mdash;they&rsquo;re both the identity function.</p>

<p>Being able to rename bound variables at will is called <strong>α-varying</strong>,
and when two terms can be made identical by just α-varying them,
we say they&rsquo;re <strong>α-equivalent</strong>.</p>

<p>It only makes sense to α-vary bound variables, not free variables. If we
have two functions like <code>λx. x + y</code> and <code>λx. x + z</code>, we can&rsquo;t safely
α-vary <code>y</code> to <code>z</code>, because we have no way of knowing whether <code>y</code> and <code>z</code>
are the same! Their same-ness depends on the context.</p>

<p>We can implement capture-avoiding substitution for the explicit variable
representation by α-varying whenever we detect that a variable might be
captured. To revisit our example from earlier:</p>

<pre><code class="python">  (λx. λy. x + y) y
→ [y / x] (λy. x + y)
  # our free 'y' will get captured by going under
  # this λ, so let's α-vary the bound 'y' to 'z':
→ [y / x] (λz. x + z)
→ λz. [y / x] (x + z)
→ λz. y + z
</code></pre>

<p>The trick here is that by picking <code>z</code> we picked a name that doesn&rsquo;t
collide with any of the free variables with in <code>λy. x + y</code>. Namely,
we&rsquo;re glad we didn&rsquo;t α-vary <code>y</code> to <code>x</code>! To ensure this, our
implementation can either</p>

<ul>
<li>calculate the set of free variables used in a subexpression and make
sure not to use one of those, or</li>
<li>generate a globally unique name by incrementing a global
counter, giving us names like <code>x1</code>, <code>x2</code>, <code>x3</code>, etc.</li>
</ul>


<p>On the surface, explicit variables look rather naïve, and maybe they
are. However, they work perfectly if you don&rsquo;t need substitution in the
first place! For example, a compiler never needs to substitute a term
for a variable in another term because compilers don&rsquo;t evaluate code:
they translate one intermediate language into another.</p>

<p>On the other hand, interpreters use term substitution heavily, and even
compilers need to substitute types for variables in other types and in
terms. We&rsquo;ll now look at some better solutions for implementing
capture-avoiding substitution.</p>

<h2>De Bruijn Indices</h2>

<p>With explicit variables, we had to keep track of names in use and check
whether to α-vary before a collision happened. The next representation
we&rsquo;ll look at sidesteps this problem by not giving names to variables at
all! Let&rsquo;s take a look at our picture from before:</p>

<pre><code class="python">                             ┌──────┐
                        λx. λy. x + y
                         └──────┘
</code></pre>

<p>In this picture, the only thing that&rsquo;s really important to us is the
binding structure; we don&rsquo;t actually care that <code>x</code> is called <code>x</code>, we
just care that applying this function sticks the argument everywhere the
line on the bottom points to. We could omit the names entirely, as long
as we can still remember where the lines should connect to:</p>

<pre><code class="python">                            ┌─────┐
                       (λ. λ. ◆ + ◆)
                         └────┘
</code></pre>

<p>One way of doing this is to count how many bindings sites up you have to
go before you arrive at the location the variable is bound. Under this
representation, variables are indices into a list of the binding sites;
we call these indices <strong>de Bruijn indices</strong>:</p>

<pre><code class="python">                       (λ. λ. ① + ⓪)
</code></pre>

<p><strong>Note</strong>: I&rsquo;m using circled numbers like <code>⓪</code> for the variable with de
Bruijn index <code>0</code>.</p>

<p>Under this representation, a de Bruijn index of 1 means &ldquo;skip over one
lambda&rdquo; and an index of 0 means &ldquo;skip over zero lambdas&rdquo; or simply &ldquo;go
to the closest lambda.&rdquo; In code, de Bruijn terms can be represented
with this datatype:</p>

<pre><code class="sml">datatype term
  = Var of int
  | Lam of term
  | App of term * term
</code></pre>

<p><code>Var</code> now takes an <code>int</code> instead of a <code>string</code>. <code>Lam</code> only takes the
body of the lambda (it used to also take a name for it&rsquo;s argument). To
refer to argument of a lambda function, we count back the appropriate
number of <code>Lam</code>s to skip over.</p>

<p>Now that all variables are represented by indices, it&rsquo;s much easier to
know which variables are free and which are bound: a variable is free if
its index is larger than the number of lambdas it&rsquo;s under.</p>

<pre><code class="python">                      ◀───────────────┐
                            ┌──────┐  │
                       (λ. λ. ① + ⓪) ③
                         └────┘
</code></pre>

<p>The <code>③</code> is free because it&rsquo;s under zero lambdas. Put another way, if we
were keeping a list of the binding sites we&rsquo;d traverse under to reach
<code>③</code> our list would be empty, so accessing index 3 would be out of
bounds.</p>

<p>With this representation, capture avoiding substitution becomes much
more manageable.</p>

<pre><code class="python">  (λ. λ. ① + ⓪) ③
→ [③ / ⓪] (λ. ① + ⓪)
# We increment free variables as we descend under binders
→ λ. [④ / ①] (① + ⓪)
→ λ. ([④ / ①] ①) + ([④ / ①] ⓪)
→ λ. ④ + ⓪
</code></pre>

<p>Note how the <code>③</code> changed to a <code>④</code>: its new location in the program lies
under one extra lambda than before. Thus to refer to the same position
as at the start of the substitution, we increment to record that we&rsquo;ll
have to skip over that extra lambda. This process of adding one when
going under a binder is called <strong>lifting</strong> (or sometimes, <strong>shifting</strong>).</p>

<p>Lifting takes the guesswork out of implementing substitution. As a
bonus, we&rsquo;ve actually forced α-equivalent terms to have identical
structure! Checking for α-equivalence is now a straightforward tree
traversal: we check that both nodes are pairwise equal, then that their
children are α-equivalent.</p>

<h2>De Bruijn Indices and Lifting</h2>

<p>On the other hand, working with de Bruijn indices can still be tricky.
It&rsquo;s easy enough to remember to lift variables when substituting, but
more generally, you have to remember to lift <em>whenever</em> you put a free
variable into a context different from where it was defined. This can
get really hairy; spotting when a usage context diverges from a
definition context is a skill that&rsquo;s often learned the hard way! Namely,
by forgetting to lift somewhere, pouring over the code and the types for
hours, then finally spotting the mistake.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></p>

<p>To make this a little more concrete, I&rsquo;ll use a specific example. It
comes from the judgement for deciding whether two type constructors are
equivalent in System F<sub>ω</sub>. Focus on the variables and contexts
in use (don&rsquo;t pay too much attention to what the judgement actually is):</p>

<p>$$
\frac{
\Gamma, \alpha :: \kappa_1 \; \vdash \; c \, \alpha \iff c&#8217; \, \alpha :: \kappa_2
}{
\Gamma \; \vdash \; c \iff c&#8217; :: \kappa_1 \to \kappa_2
}
$$</p>

<p>In words, &ldquo;to check whether type constructors <code>c</code> and <code>c'</code> are
equivalent, assume that <code>α</code> is a type constructor of kind <code>κ₁</code>, then
apply <code>α</code> to <code>c</code> and <code>c'</code> and see if you get the same result in both
cases.&rdquo; Though again, understanding this judgement is beside the point.</p>

<p>The real tricky part here is obscured by the fact that we&rsquo;re
representing variables with names instead of de Bruijn indices. If we
were to take a naive pass at translating  this rule to use de Bruijn
indices, we might end up with:</p>

<p>$$
\frac{
\Gamma, \kappa_1 \; \vdash \; c \, ⓪ \iff c&#8217; \, ⓪ :: \kappa_2
}{
\Gamma \; \vdash \; c \iff c&#8217; :: \kappa_1 \to \kappa_2
}
$$</p>

<p>Note how <code>Γ</code> became a stack instead of a map because we&rsquo;re mapping
<em>indices</em> to kinds (instead of string keys to kinds). The element we
just pushed on (<code>κ₁</code>) is on the top of the stack at index <code>0</code>, and
everything else in the context can now be found at <code>index + 1</code>. That
means that above the line, <code>⓪</code> refers to the <code>κ₁</code> in the context.</p>

<p>But <code>⓪</code> might <em>also</em> be in use at the top level of <code>c</code> or <code>c'</code>! In
either of these terms, <code>⓪</code> at the top level is a free variable referring
to the first thing in <code>Γ</code>. The problem is that we&rsquo;re checking <code>c ⓪ &lt;=&gt;
c' ⓪</code> with context <code>Γ, κ₁</code> rather than <code>Γ</code>, so all our indices are off.</p>

<p>To make all the indices in <code>c</code> and <code>c'</code> route to the correct variable in
the new context, we have to go through <code>c</code> and <code>c'</code> and lift all free
variables by one to reflect the fact that we just injected something
into the surrounding context:</p>

<p>$$
\frac{
\Gamma, \kappa_1 \; \vdash \; (c \uparrow) \, ⓪ \iff (c&#8217; \uparrow) \, ⓪ :: \kappa_2
}{
\Gamma \; \vdash \; c \iff c&#8217; :: \kappa_1 \to \kappa_2
}
$$</p>

<p>In this rule, <code>↑</code> is the lifting operator, which traverses through a
term&rsquo;s free variables and increments them. After it&rsquo;s run, there will be
no free variables in <code>c</code> or <code>c'</code> with index 0, which gives us room to
use <code>⓪</code> for our own purposes.</p>

<p>In some sense, this is the opposite problem that we had when we used
explicit variables. For that, we had to go through and rename <em>bound
variables</em> so that nothing clashed. Now, we have to lift <em>free
variables</em> so that nothing clashes. Put another way, explicit variables
excel at dealing with free variables, while de Bruijn indices excel at
representing bound variables.</p>

<p>The next representation we&rsquo;ll look at, locally nameless terms,
effectively steals the best of each, combining them into one
representation.</p>

<h2>Locally Nameless Terms</h2>

<p>We identified that de Bruijn indices represent bound variables well at
the expense of free variables. <strong>Locally nameless terms</strong> solve this by
giving free variables explicit names, but using indices instead of names
for bound (or &ldquo;local&rdquo;) variables, thus the name.<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup></p>

<p>Locally nameless terms might be represented by a data type like this:</p>

<pre><code class="sml">datatype term
  = FV of string
  | BV of int
  | Lam of term
  | App of term * term
</code></pre>

<p><code>FV</code> constructs a free variable, and similarly <code>BV</code> constructs a bound
variable. <code>FV</code> takes a <code>string</code>, because free variables get names, and
<code>BV</code> takes an <code>int</code>, because bound variables are nameless de Bruijn
indices. As before, <code>Lam</code> only takes the body of the lambda function;
we&rsquo;ll use de Bruijn indices to count back to the appropriate binding
site of a variable.</p>

<p>In practice, locally nameless terms are best provided through a library,
where this internal implementation is hidden and the user interacts with
an abstract interface:</p>

<pre><code class="sml">(* The actual type of a locally nameless term with *)
(* a distinction between FV and BV is hidden       *)
type term

(* termView is only one level deep: after that, *)
(* you end up with a term, which is abstract    *)
datatype termView
  = Var of string
  | Lam of string * term
  | App of term * term

(* Convert between the abstract and view types *)
val out : term -&gt; termView
val into : termView -&gt; term

(* Substitution and alpha equivalence work on abstract terms *)
val subst : term -&gt; string -&gt; term
val aeq : term -&gt; term -&gt; bool
</code></pre>

<p>The fresh name generation from explicit variables is handled under the
hood by <code>out</code>. Lifting is handled automatically every time we call
<code>into</code> on a <code>Lam</code>. By only implementing operations like <code>subst</code> and
<code>aeq</code> on the abstract representations, we&rsquo;ve effectively forced the type
system to check that we lift and generate fresh names in all the right
places!</p>

<h2>Closing Considerations</h2>

<p>Locally nameless terms are generally pretty great. They blend the
strengths of explicit variables and de Bruijn indices into a new
structure that makes working with variables and binding hard to get
wrong. That being said, I&rsquo;d be remiss if I didn&rsquo;t point out two
drawbacks:</p>

<ul>
<li>Locally nameless terms can be slow.

<ul>
<li>In most code, we&rsquo;ll find ourselves converting between <code>term</code>s
and <code>termView</code>s. This brings with it the overhead of the function
call, allocating new memory for the new structures, and can even
sometimes make a linear algorithm accidentally quadratic.</li>
</ul>
</li>
<li>It&rsquo;s more annoying to use pattern matching.

<ul>
<li>Most of the time we&rsquo;ll have things of type <code>term</code>. Since <code>term</code> is
abstract, we can&rsquo;t pattern match on it directly; we have to instead
call <code>out</code> and pattern match the result.</li>
</ul>
</li>
</ul>


<p>Despite these drawbacks, I still prefer locally nameless terms.</p>

<ul>
<li><p>I&rsquo;ll gladly trade correctness for performance, and it&rsquo;s definitely
easier to be correct when working with locally nameless terms. We can
always optimize for performance later by profiling the code to find
the slowness!</p></li>
<li><p>Calling <code>out</code> in a few places is a small ergonomic price to pay for
correctness. When you forget to call <code>out</code> or <code>into</code>, the type checker
will remind you. There are also some cool language extensions which
can make calling <code>out</code> and <code>in</code> syntactically more pleasant, like
<a href="https://ocharles.org.uk/blog/posts/2014-12-02-view-patterns.html">View Patterns</a> in Haskell.</p></li>
</ul>


<p>Variables show up in the most interesting places, and I always smile
when I find them being used in new and surprising ways. On the flip
side, languages that don&rsquo;t implement variables and binding suffer no end
of trouble and programmers are forced to cope with their
absence.<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup></p>

<p>I think variables are just so cool!</p>

<!-- vim:tw=72
-->

<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>Nearly every interesting programming language feature derives its power from variables. Functions wouldn&rsquo;t be functions if not for variables. Modularity and linking reduce to variables and substitution. I&rsquo;ve written in the past about all sorts of cool <a href="/variables-in-types">variables in types</a>, as well as how <a href="/system-f-param">parametric polymorphism in System F</a> is the result of using type variables in two ways within the same system.<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p>If it wasn&rsquo;t clear, this has happened to me many times, and yes I&rsquo;m still getting over it 😓<a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
<li id="fn:3">
<p>We could, by analogy, refer to the de Bruijn index representation as the globally nameless representation, which is more descriptive but isn&rsquo;t something you&rsquo;ll hear used anywhere.<a href="#fnref:3" rev="footnote">&#8617;</a></p></li>
<li id="fn:4">
<p>It&rsquo;s for this very reason that variables are the first topic we cover in <a href="https://www.cs.cmu.edu/~rwh/courses/ppl/">15-312 Principles of Programming Languages</a>.<a href="#fnref:4" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[System Fω and Parameterization]]></title>
    <link href="https://blog.jez.io/system-f-param/"/>
    <updated>2017-09-27T19:14:45-07:00</updated>
    <id>https://blog.jez.io/system-f-param</id>
    <content type="html"><![CDATA[<p>When I first learned about System F<sub>ω</sub>, I was confused about
the difference between <code>∀(t.τ)</code> (forall types) and <code>λ(u.c)</code> (type
abstractions) for a long time, but recently I finally grasped the
difference! Both of these constructs have to do with parameterization
(factoring out a variable so that it&rsquo;s bound), but the two types have
drastically different meanings.</p>

<!-- more -->


<h2>Questions</h2>

<p>We&rsquo;ll start off with some questions to keep in mind throughout these
notes. Our goals by the end are to understand what the questions are
asking, and have at least a partial&mdash;if not complete&mdash;answer to each.</p>

<p>First, consider this code.</p>

<pre><code class="sml">datatype 'a list = Nil | Cons of 'a * 'a list
</code></pre>

<ul>
<li>What really is &ldquo;<code>list</code>&rdquo; in this code?</li>
<li>Or put another way, how would we define <code>list</code> in System
F<sub>ω</sub>?</li>
</ul>


<p>Thinking more broadly,</p>

<ul>
<li>What separates <code>∀(t.τ)</code> and <code>λ(u.c)</code>?</li>
<li>What is parameterization, and how does it relate to these things?</li>
</ul>


<h2>System F<sub>ω</sub></h2>

<p>The answers to most of these questions rely on a solid definition of
System F<sub>ω</sub>. We&rsquo;ll be using this setup.</p>

<pre><code>Kind κ ::= * | κ → κ | ···

           abstract       concrete      arity/valence
Con c  ::= ···
         | arr(c₁; c₂)    c₁ → c₂       (Con, Con)Con
         | all{κ}(u.c)    ∀(u ∷ κ). c   (Kind, Con.Con)Con
         | lam{κ}(u.c)    λ(u ∷ κ). c   (Kind, Con.Con)Con
         | app(c₁; c₂)    c₁(c₂)        (Con, Con)Con
</code></pre>

<p>Some points to note:</p>

<ul>
<li><code>∀(u ∷ κ). c</code> and <code>λ(u ∷ κ). c</code> have the same arity.</li>
<li><code>∀(u ∷ κ). c</code> and <code>λ(u ∷ κ). c</code> both <em>bind</em> a constructor variable.
This makes these two operators <em>parametric</em>.</li>
<li>Only <code>λ(u ∷ κ). c</code> has a matching elim form: <code>c₁(c₂)</code>.
(There are no elim forms for <code>c₁ → c₂</code> and <code>∀(u ∷ κ). c</code>, because they
construct types of kind <code>*</code>. This will be important later.)</li>
</ul>


<p>It&rsquo;ll also be important to have these two inference rules for kinding:</p>

<p>$$
\frac{
  \Delta, u :: \kappa \vdash c :: *
}{
  \Delta \vdash \forall(u :: \kappa). \, c :: *
}\;(\texttt{forall-kind})
$$
$$
\frac{
  \Delta, u :: \kappa \vdash c :: \kappa&#8217;
}{
  \Delta \vdash \lambda(u :: \kappa). \, c :: \kappa \to \kappa&#8217;
}\;(\texttt{lambda-kind})
$$</p>

<h2>Defining the <code>list</code> Constructor</h2>

<p>Let&rsquo;s take another look at this datatype definition from above:</p>

<pre><code class="sml">datatype 'a list = Nil | Cons of 'a * 'a list
</code></pre>

<p>We&rsquo;ve <a href="/variables-in-types/">already seen</a> how to encode the type of lists
of integers using inductive types:</p>

<pre><code>intlist = μ(t. 1 + (int × t))
</code></pre>

<p>Knowing what we know about System F (the &ldquo;<strong>polymorphic</strong> lambda
calculus&rdquo;), our next question should be &ldquo;how do we encode
<strong>polymorphic</strong> lists?&rdquo; Or more specifically, which of these two
operators (<code>λ</code> or <code>∀</code>) should we pick, and why?</p>

<p>First, we should be more specific, because there&rsquo;s a difference between
<code>list</code> and <code>'a list</code>. Let&rsquo;s start off with defining <code>list</code> in
particular. From what we know of programming in Standard ML, we can do
things like:</p>

<pre><code class="sml">(* Apply 'int' to 'list' function! *)
type grades = int list

type key = string
type val = real

(* Apply '(key, val)' to 'list' function! *)
type updates = (key, val) list
</code></pre>

<p>If we look really closely, what&rsquo;s actually happening here is that <code>list</code>
is a type-level <em>function</em> that returns a type (and we use the <code>type foo
= ...</code> syntax to store that returned type in a variable).<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p>

<p>Since <code>list</code> is actually a function from types to types, it must have
an arrow kind: <code>* → *</code>. Looking back at our two inference rules for
kinding, we see only one rule that lets us introduce an arrow kind: <code>λ(u
∷ κ). c</code>. On the other hand, <code>∀(u ∷ κ). c</code> must have kind <code>*</code>; it
<em>can&rsquo;t</em> be used to define type constructors.</p>

<p>Step 1: define list constructor? Check:</p>

<pre><code>list = λ(α ∷ *). μ(t. 1 + (α × t)))
</code></pre>

<h2>Defining Polymorphic Lists</h2>

<p>It doesn&rsquo;t stop with the above definition, because it&rsquo;s still not
<em>polymorphic</em>. In particular, we can&rsquo;t just go write functions on
polymorphic lists with code like this:</p>

<pre><code class="sml">fun foo (x : list) = (* ··· *)
</code></pre>

<p>We can&rsquo;t say <code>x : list</code> because all intermediate terms in a given
program have to type check as a type of kind <code>*</code>, whereas <code>list ∷ * →
*</code>. Another way of saying this: there isn&rsquo;t any way to introduce a value
of type <code>list</code> because there&rsquo;s no way to introduce values with arrow
kinds.</p>

<p>Meanwhile, we <em>can</em> write this:</p>

<pre><code class="sml">fun foo (x : 'a list) = (* ··· *)
</code></pre>

<p>When you get down to it, this is actually kind of weird. Why is it okay
to use <code>'a list</code>? I never defined <code>'a</code> anywhere, so wouldn&rsquo;t that make
it an unbound variable?</p>

<p>It turns out that when we use type variables like this, SML
automatically binds them for us by inserting <code>∀</code>s into our code. In
particular, it implicitly infers a type like this:</p>

<pre><code class="sml">val foo : forall 'a. 'a list -&gt; ()
</code></pre>

<p>SML inserts this <code>forall</code> automatically because its type system is a bit
less polymorphic than System F<sub>ω</sub>&rsquo;s. Some might call this a
drawback, though it does save us from typing <code>forall</code> annotations
ourselves. And really, for most anything else we&rsquo;d call a &ldquo;drawback&rdquo; of
this design, SML makes up the difference with modules.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></p>

<p>Step 2: make polymorphic list for use in annotation? Check:</p>

<pre><code>α list = ∀(α ∷ *). list(α)
</code></pre>

<h2>Variables &amp; Parameterization</h2>

<p>Tada! We&rsquo;ve figured out how to take a list datatype from SML and encode
it in System F<sub>ω</sub>, using these two definitions:</p>

<pre><code>  list = λ(α ∷ *). μ(t. 1 + (α × t)))
α list = ∀(α ∷ *). list(α)
</code></pre>

<p>We could end here, but there&rsquo;s one more interesting point. If we look
back, we started out with the <code>∀</code> and <code>λ</code> operators having the same
arity, but somewhere along the way their behaviors differed. <code>λ</code> was
used to create type constructors, while <code>∀</code> was used to introduce
polymorphism.</p>

<p>Where did this split come from? What distinguishes <code>∀</code> as being the
go-to type for polymorphism, while <code>λ</code> makes type constructors
(type-to-type functions)? Recall one of the earliest ideas we teach in
<a href="http://www.cs.cmu.edu/~rwh/courses/ppl/">15-312</a>:</p>

<blockquote><p>&hellip; the core idea carries over from school mathematics, namely
that <strong>a variable is an unknown, or a place-holder, whose meaning is
given by substitution.</strong></p>

<p>&ndash; Harper, <em>Practical Foundations for Programming Languages</em></p></blockquote>

<p>Variables are given meaning by substitution, so we can look to the
appropriate substitutions to uncover the meaning and the differences
between <code>λ</code> and <code>∀</code>. Let&rsquo;s first look at the substitution for <code>λ</code>:</p>

<p>$$
\frac{
  \Delta, u :: \kappa_1 \vdash c_2 :: \kappa_2 \qquad \Delta \vdash c_1
  :: \kappa_1
}{
  \Delta \vdash (\lambda(u :: \kappa_1). \, c_2)(c_1) \equiv  [c_1/u]c_2 :: \kappa_2
}
$$</p>

<p>We can think of this as saying &ldquo;when you apply one type to another, the
second type gets full access to the first type to construct a new type.&rdquo;
We notice that the substitution here is completely <strong>internal to the
type system</strong>.<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup></p>

<p>On the other hand, the substitution for <code>∀</code> <strong>bridges the gap</strong> from
types to terms:</p>

<p>$$
\frac{
  \Delta \, \Gamma, e : \forall (u :: \kappa). \tau \qquad \Delta \vdash c :: \kappa
}{
  \Delta \, \Gamma \vdash e[c] : [c/u]\tau
}
$$
$$
\frac{
  \mbox{}
}{
  (\Lambda u. \, e)[\tau] \mapsto [\tau / u]e
}
$$</p>

<p>When we&rsquo;re type checking a polymorphic type application, we don&rsquo;t get to
know anything about the type parameter <code>u</code> other than its kind. But when
we&rsquo;re running a program and get to the evaluation of a polymorphic type
application, we substitute the concrete <code>τ</code> directly in for <code>u</code> in <code>e</code>,
which bridges the gap from the type-level to the term-level.</p>

<p>At the end of the day, all the interesting stuff came from using
functions (aka, something parameterized by a value) in cool ways. Isn&rsquo;t
that baffling? Functions are so powerful that they seem to always pop up
at the heart of the most interesting constructs. I think it&rsquo;s
straight-up amazing that something so simple can at the same time be
that powerful. Functions!</p>

<!-- vim:tw=72
-->

<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>It&rsquo;s easy to not notice at first that type definitions are really function calls because in Standard ML, the type function applications are backwards. Instead of <code>f(x)</code>, it&rsquo;s <code>x f</code>. This is more similar to how we actually think when we see a function. Consider <code>h(g(f(x)))</code> (or another way: <code>h . g . f $ x</code>). We read this as &ldquo;take x, do f, pass that to g, and pass that to h&rdquo;. Why not write <code>x f g h</code> in the first place?<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p>Other languages (like Haskell or PureScript) have a language feature called &ldquo;Rank-N Types&rdquo; which is really just a fancy way of saying &ldquo;you can put the <code>forall a.</code> anywhere you want.&rdquo; Oftentimes, this makes it harder for the compiler to infer where the variable bindings are, so you sometimes have to use more annotations than you might if you weren&rsquo;t using Rank-N types.<a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
<li id="fn:3">
<p>It&rsquo;s not super relevant to this discussion, but this inference rule is for the judgement defining equality of type constructors. This comes up all over the place when you&rsquo;re writing a compiler for SML. If this sounds interesting, definitely take 15-417 HOT Compilation!<a href="#fnref:3" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Variables in Types]]></title>
    <link href="https://blog.jez.io/variables-in-types/"/>
    <updated>2017-09-25T02:17:31-07:00</updated>
    <id>https://blog.jez.io/variables-in-types</id>
    <content type="html"><![CDATA[<p>These are some recitation notes for an ad-hoc recitation I gave for the
class <a href="http://www.cs.cmu.edu/~rwh/courses/ppl/">15-312 Principles of Programming Languages</a>. It was probably
my favorite 312 recitation content-wise, because it&rsquo;s the first
recitation where we&rsquo;ve covered enough stuff to where we can really start
connecting the dots.</p>

<p><strong>Abstract</strong>:</p>

<blockquote><p>We&rsquo;ve seen a number of examples in class of types which use variables.
Having variables in our type systems lends a great deal of power to
languages using these type systems. We&rsquo;re going to look at how
variables are used in generic programming, inductive &amp; coinductive
types, and polymorphic types.</p></blockquote>

<h3>→ <a href="/notes/variables-in-types.pdf">Variables in Types</a></h3>

<!-- vim:tw=72
-->

]]></content>
  </entry>
  
</feed>
